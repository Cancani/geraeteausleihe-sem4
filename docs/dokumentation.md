# Projektdefinition

## Einreichungsformular

Das Einreichungsformular kann hier entnommen werden:  
[ITCNE24 Semesterarbeit 4 Einreichungsformular](./Efekan_Einreichungsformular_4.SemesterV2.docx)

---

## Projekt√ºbersicht

| Eigenschaft | Details |
|---|---|
| **Titel** | Ger√§teausleihe Microservice, Cloud Native Deployment auf AWS |
| **Studierender** | Efekan Demirci |
| **Dozenten** | PRJ Corrado Parisi, MSVC Philip Stark |
| **Semester** | 4 Semester HF TBZ, ITCNE24 |
| **Zeitraum** | Oktober 2025 bis Januar 2026 |
| **Technologie Stack** | Python Flask, Docker, K3s auf AWS EC2, GitHub Actions, GHCR, MkDocs |
| **Architektur** | Microservice, Container, Kubernetes Deployment, Ingress Routing |
| **Repository** | https://github.com/Cancani/geraeteausleihe-sem4 |
| **GitHub Pages** | https://cancani.com/geraeteausleihe-sem4/ |
| **Project Board** | https://github.com/users/Cancani/projects/3/views/1 |

Die bestehende Ger√§teausleihe L√∂sung aus der vorherigen Semesterarbeit dient als Ausgangsbasis. In dieser Arbeit steht nicht die fachliche Erweiterung im Vordergrund, sondern die Cloud Native Transformation: Containerisierung, automatisierte Build und Deploy Pipelines sowie der Betrieb in einem Kubernetes Cluster auf AWS.

**Key Features**
* Container Build und Push nach GHCR
* Automatisiertes Deployment nach K3s auf AWS EC2 via GitHub Actions
* Ingress Routing ueber Traefik und Hostname ueber nip.io
* Extern erreichbare Endpoints healthz und pdf
* Laufende Dokumentation ueber MkDocs und GitHub Pages

---

## Ausgangslage und Problemstellung

### 1.3.1 Ist Zustand

Der Service war zwar technisch lauff√§hig, der Betrieb und das Deployment waren jedoch zu wenig standardisiert und zu wenig automatisiert. Zudem fehlten laufende Nachweise und eine klare Transparenz √ºber Projektstand, Risiken und Fortschritt.

Typische Nachteile im Ist Zustand:
* Deployment Updates waren fehleranf√§llig und schwer nachvollziehbar
* Kein konsistentes Release und Tagging Konzept
* Keine Kubernetes Eigenschaften wie Self Healing und deklarative Deployments
* Nachweise f√ºr Stakeholder fehlten oder wurden erst sp√§t nachgetragen

**Ist Workflow**

```mermaid
flowchart LR
  A[Lokale Aenderung am Code] --> B[Manueller Build oder uneinheitlicher Build]
  B --> C[Manuelles Deployment]
  C --> D[Service laeuft, Status schwer nachvollziehbar]
```

---

### 1.3.2 Soll Zustand

Ziel ist ein durchg√§ngig automatisierter Workflow vom Commit bis zum laufenden Pod im Cluster. Jede relevante √Ñnderung soll ein Image bauen, in die Registry pushen und anschliessend automatisiert im K3s Cluster deployed werden. Der Betriebszustand soll jederzeit √ºber Board, Doku und konkrete Nachweise nachvollziehbar sein.

**Soll Workflow**

```mermaid
flowchart LR
  subgraph GitHub
    A[Repository] --> B[GitHub Actions]
    B --> C[GHCR Image]
    B --> D[GitHub Pages Doku]
  end

  subgraph AWS
    E[AWS EC2] --> F[K3s Cluster]
    F --> G[Traefik Ingress]
    G --> H[Microservice Pod]
  end

  C --> H
  B --> E
```

**Soll Verbesserungen**

| Verbesserung | Nutzen | Umsetzung |
|---|---|---|
| Automatisierter Build und Push | Reproduzierbar, nachvollziehbar | GitHub Actions, GHCR |
| Automatisierter Deploy | Schneller und konsistent | GitHub Actions, kubectl apply |
| Deklarative Kubernetes Manifeste | Standardisiertes Deployment | Namespace, Deployment, Service, Ingress |
| Laufende Nachweise | Transparenz f√ºr Stakeholder | Screenshots, Links, Sprint Reviews |

---

## Zielsetzung

Die Arbeit hat das Ziel, den bestehenden Microservice in eine Cloud Native Betriebsumgebung zu √ºberfuehren. Der Fokus liegt auf CI und CD, Kubernetes Deployment, stabiler Erreichbarkeit ueber Ingress sowie einer laufenden Dokumentation, die den Fortschritt und die Qualit√§t belegt.

---

### 1.4.1 SMART Ziele

| Ziel | Spezifisch | Messbar | Attraktiv | Realistisch | Terminiert |
|---|---|---|---|---|---|
| CI Build und Push | Container Image wird gebaut und nach GHCR gepusht | Erfolgreiche Actions Runs und sichtbare Tags | DevOps Nutzen | Mit GitHub Actions umsetzbar | Sprint 2 und 3 |
| CD Deploy nach K3s | Manifeste werden applied und Image wird aktualisiert | Pods laufen, Service extern erreichbar | Automatisierung | Mit K3s auf EC2 umsetzbar | Sprint 2 und 3 |
| Ingress und Endpoints | Health und PDF funktionieren extern | HTTP 200, PDF Response | Demo faehig | Traefik und nip.io vorhanden | Sprint 2 |
| Dokumentation und Nachweise | Laufende Sprint Reviews mit Belegen | Pro Sprint Belegliste mit Screenshots | Stakeholder Transparenz | Mit MkDocs umsetzbar | Laufend |

---

## Technologien und Werkzeuge

| Bereich | Technologie | Begr√ºndung |
|---|---|---|
| Backend | Python Flask | Schlankes Framework f√ºr Microservices |
| Containerisierung | Docker | Portabilit√§t und reproduzierbarer Betrieb |
| Orchestrierung | K3s | Kubernetes Betrieb auf einer EC2 Instanz |
| Cloud Hosting | AWS EC2 | Realistische Zielumgebung f√ºr DevOps Deployment |
| Registry | GHCR | In GitHub integriert, einfache Distribution |
| CI und CD | GitHub Actions | Automatisierung von Build, Push und Deploy |
| Ingress | Traefik | Routing im Cluster und externe Erreichbarkeit |
| Dokumentation | MkDocs, GitHub Pages | Versionierte und laufend publizierte Doku |
| Projektmanagement | GitHub Projects, Issues | Backlog, Sch√§tzungen, Priorisierung, DoD |

# Projektmanagement

## Projektmethodik

Das Projekt folgt einem agilen, scrum√§hnlichen Vorgehen mit iterativer Entwicklung und regelm√§ssigen Review Zyklen. Die Planung und Nachverfolgung erfolgt vollst√§ndig in GitHub.

**Gew√§hlte Methodik: Sprint basierte Entwicklung**

Die Entscheidung f√ºr ein iteratives Vorgehen basiert auf folgenden Punkten:

* Neue technische Themen wie Kubernetes, K3s und CI CD ben√∂tigen experimentelles Vorgehen mit kurzen Feedback Schleifen
* Technische Abh√§ngigkeiten werden oft erst w√§hrend der Umsetzung sichtbar, zum Beispiel Ingress, Host Routing und Secrets Handling
* Dozenten Feedback kann direkt in die n√§chsten Tasks und in die Dokumentation einfliessen
* Risiken werden fr√ºh sichtbar, statt erst am Schluss

**Kernprinzipien der angewandten Methodik**

* Iterative Entwicklung mit funktionsf√§higen Zwischenst√§nden
* Kontinuierliches Feedback und Anpassung der Priorit√§ten
* Laufende Nachweisf√ºhrung, damit der Projektstand jederzeit nachvollziehbar ist
* Klare Definition of Done pro Ticket inklusive Evidence Anforderungen

### 2.1.1 Sprintstruktur im Detail

**Sprint Planning (Sprintbeginn):**
* Definition von User Stories mit klaren Akzeptanzkriterien
* Aufwandssch√§tzung in Story Points und Priorisierung nach Must Should Could
* Festlegung des Sprintziels als ein Satz Outcome und der Deliverables
* Sprint Scope im GitHub Project Board zuweisen, Sprint Feld und Milestone setzen

**Sprint Execution (Durchf√ºhrung):**
* Kontinuierliche Arbeit an den definierten User Stories
* GitHub Issues f√ºr Aufgaben Tracking und Statusupdates
* Regelm√§ssige Commits und Pushes, kleine √Ñnderungen statt grosse Spr√ºnge
* Ticket Status aktuell halten, WIP Limit in Progress maximal 2

**Sprint Review (Sprintende):**
* Demo der implementierten Features und Abgleich gegen Sprintziel
* Review Gespr√§che mit Dozenten zur Qualit√§tssicherung
* Bewertung der Zielerreichung und Identifikation von Verbesserungspotenzialen
* Evidence Pflicht, Screenshots und Links werden direkt pro Sprint Review dokumentiert

**Sprint Retrospektive:**
* Reflexion des Arbeitsprozesses mit dem Starfish Modell
* Identifikation von Start Doing, Stop Doing, Keep Doing, More Of, Less Of
* Konkrete Action Items als Issues erfassen und dem n√§chsten Sprint zuordnen

**Vorteile der gew√§hlten Methodik:**
* Flexibilit√§t, schnelle Anpassung an neue Erkenntnisse, zum Beispiel Ingress Routing oder CI CD Details
* Qualit√§tssicherung, regelm√§ssige Reviews verhindern sp√§te Richtungs√§nderungen
* Motivation, sichtbare Fortschritte nach jedem Sprint
* Lernoptimierung, Retrospektiven f√ºhren zu kontinuierlicher Prozessverbesserung

## Projektphasen und Meilensteine

Das Projekt ist in drei Sprints gegliedert. Die Sprintzeitr√§ume entsprechen der urspr√ºnglichen Planung. Inhalte aus Sprint 1 und Sprint 2 wurden in einem sp√§teren Zeitraum konzentriert nachgezogen. Die Dokumentation wird nun strukturiert und evidenzbasiert erg√§nzt, damit der Stand jederzeit nachvollziehbar ist.

### 2.2.1 Sprint Progression im √úberblick

**Sprint 1 Projektbasis:**  
Aufbau der Projektbasis mit Board Struktur, Labels, Milestones, Issue Standards und erstem Architektur Zielbild.

**Sprint 2 Cluster und Deploy:**  
Aufbau der Laufzeitumgebung auf AWS EC2 mit K3s, Kubernetes Ressourcen und Ingress √ºber Traefik. Erste externe Tests der Endpoints.

**Sprint 3 CI CD und Nachweise:**  
Stabilisierung von Build und Deploy Workflows mit GitHub Actions, saubere Versionierung, Rollback Vorgehen und laufende Nachweise pro Sprint.

![Sprintuebersicht](./screenshots/Sprintuebersicht.png)

### 2.2.2 Zeitplan

| Sprint       | Zeitraum                  | Fokus                                                                                      | Status        |
| ------------ | ------------------------- | ------------------------------------------------------------------------------------------ | ------------- |
| **Sprint 1** | 27.10.2025 bis 17.11.2025 | Projektbasis, Board Setup, Standards, Architektur Zielbild                                 | Abgeschlossen |
| **Sprint 2** | 18.11.2025 bis 15.12.2025 | AWS EC2 und K3s Setup, Kubernetes Manifeste, Ingress, erste externe Tests                  | Abgeschlossen |
| **Sprint 3** | 16.12.2025 bis 13.01.2026 | CI CD stabilisieren, Evidence pro Sprint, Dokumentation Finalisierung, Abgabe Vorbereitung | In Arbeit     |


---

## Anpassung der Sprint Planung

In der praktischen Umsetzung wurden zentrale Inhalte aus Sprint 1 und Sprint 2 in einem sp√§teren Zeitraum konzentriert nachgezogen. Gr√ºnde daf√ºr waren parallele Verpflichtungen und die Priorisierung des technischen Fortschritts vor der finalen Dokumentationsform.

Die Sanierung erfolgt durch folgende Massnahmen:

* Backlog wird konsequent priorisiert und gesch√§tzt
* Definition of Done wird strikt eingehalten, closed Tickets werden bei offenen DoD Punkten wieder ge√∂ffnet
* Sprint Reviews enthalten ab jetzt konkrete Nachweise, nicht nur Text
* Status Updates erfolgen regelm√§ssig, inklusive Links auf Board und Dokumentation

---

## Issues und User Stories

Das Projekt umfasst 27 User Stories, US01 bis US27. Alle Stories werden als GitHub Issues gef√ºhrt und im [GitHub Project Board](https://github.com/users/Cancani/projects/3) verwaltet.

**Standards pro Issue**

* User Story Text
* Akzeptanzkriterien als Checkboxen
* Definition of Done als Checkboxen
* Labels f√ºr Priorit√§t und Bereich
* Milestone Zuordnung zu Sprint

**Project Board Felder**

Die Steuerung erfolgt √ºber folgende Felder im GitHub Project:

| Feld | Zweck |
|---|---|
| Status | Backlog, Ready, In Progress, Review, Done |
| Story Points | Aufwandssch√§tzung |
| Priorit√§t | Must, Should, Could |
| Sprint | Sprint 1, Sprint 2, Sprint 3 |
| DoD erf√ºllt | Ja, Nein |

**Board Workflow**

| Spalte | Bedeutung |
|---|---|
| Backlog | Neue Anforderungen, noch nicht priorisiert |
| Ready | Priorisiert und bereit zur Umsetzung |
| In Progress | Aktive Umsetzung, WIP Limit beachten |
| Review | DoD Kontrolle, Evidence pr√ºfen |
| Done | Abgeschlossen und dokumentiert |

---

## Sprint 1 Planung und Sprint Review

### 2.5.1 Sprint 1 Planung

**Sprint Zeitraum**

27.10.2025 bis 17.11.2025

**Sprint Goal**

Projektbasis schaffen, damit Fortschritt und Qualit√§t transparent nachvollziehbar sind. Fokus liegt auf Board Setup, Standards und erstem Architektur Zielbild.

![Sprint 1 Milestone & Issues](./screenshots/Sprint1_1.png)

**Sprint 1 Scope**

Die folgenden User Stories geh√∂ren zu Sprint 1:

| US | Titel | Bereich |
|---|---|---|
| US01 | Kanban Board finalisieren | Projektmanagement |
| US02 | Labels anlegen | Projektmanagement |
| US03 | Milestones anlegen | Projektmanagement |
| US04 | Issue Template einrichten | Projektmanagement |
| US05 | Branching Strategie dokumentieren | Dokumentation |
| US06 | Sprint 1 Review und Retro dokumentieren | Dokumentation |
| US07 | Architektur Zielbild skizzieren | Architektur |

**WIP Regel**

In Progress maximal 2 parallel laufende Issues.

**Evidence Standard f√ºr Sprint 1**

F√ºr Sprint 1 werden mindestens folgende Nachweise geplant:

* Screenshot Project Board √úbersicht
* Screenshot Labels
* Screenshot Milestones
* Screenshot Issue Template
* Link zur Branching Dokumentation
* Architektur Zielbild als Diagramm

---

### 2.5.2 Sprint 1 Review

**Review Ergebnis**

Sprint 1 wurde umgesetzt. Die Projektbasis ist vorhanden und bildet die Grundlage f√ºr Sprint 2 und Sprint 3.

| Review Punkt | Ergebnis |
|---|---|
| Board Struktur vorhanden und nachvollziehbar | Erf√ºllt |
| Labels vorhanden und konsistent genutzt | Erf√ºllt |
| Milestones f√ºr Sprints vorhanden | Erf√ºllt |
| Issue Template mit Akzeptanzkriterien und DoD vorhanden | Erf√ºllt |
| Branching Strategie dokumentiert | Erf√ºllt |
| Architektur Zielbild skizziert | Erf√ºllt |

![Sprint 2 erledigt](./screenshots/Sprint1_2.png)

### Nachweise und Belege


#### Board und Planung
* Project Board Overview  
  ![Project Board Sprint 1](./screenshots/Projectboard_Sprint_1.png)

* Labels  
  ![Issue Labels](./screenshots/Issue_Labels.png)

* Milestones  
  ![Milestones](./screenshots/Milestones_Uebersicht.png)

#### Standards
* Issue Template  
  ![Issue Template](./screenshots/Issue_Template_1.png)
  ![Issue Template 2](./screenshots/Issue_Template_2.png)

* Branching Strategie Dokumentation  
  Link: 

#### Architektur

Link:

## Verwaltung der Aufgaben

Die Aufgaben wurden vollst√§ndig in [GitHub Projects](https://github.com/users/Cancani/projects/3/views/1) organisiert.  
Das Board ist nach Sprints gegliedert und orientiert sich an der Kanban-Struktur mit den Spalten  
**Backlog**, **Ready**, **In Progress**, **Review** und **Done**.  

Jede Aufgabe ist als **GitHub Issue** (US01 ‚Äì US27) angelegt und enth√§lt:
- eine klare **User Story**,
- **Akzeptanzkriterien** und **Definition of Done** als Checkboxen,
- eine **Priorit√§t** (Must / Should / Could),
- die Zuordnung zum **Sprint** als Milestone


![Project Board](./screenshots/Projectboard_Sprint_1.png)

Der Fortschritt ist √ºber das Board jederzeit nachvollziehbar:  
Geschlossene Issues wandern automatisch nach ‚ÄûDone‚Äú, offene bleiben in ‚ÄûReview‚Äú, bis alle DoD-Kriterien erf√ºllt und Nachweise in der Dokumentation erg√§nzt sind.  
Diese Struktur sorgt f√ºr durchg√§ngige Transparenz im gesamten Projektmanagementprozess.

---

## Wechsel von Microsoft Planner zu GitHub Projects und Issues

### 2.7.1 Hintergrund

Zu Beginn der Semesterarbeit 4 wurde die Projektplanung zun√§chst provisorisch in Microsoft Planner vorbereitet, da das Tool visuell bekannt war und bereits in fr√ºheren Arbeiten genutzt wurde.  
Mit dem Start der technischen Umsetzung erfolgte jedoch der Umstieg auf **GitHub Projects**, um Code, Doku und Aufgabenverwaltung auf einer Plattform zu b√ºndeln.  

Durch diesen Wechsel konnten alle User Stories direkt mit Commits, Pull Requests und Actions-Runs verkn√ºpft werden.  
Damit war es erstmals m√∂glich, Planung, Automatisierung und Deployment vollst√§ndig integriert zu f√ºhren.

Der Entscheid wurde im Sprint-Review mit Corrado Parisi und Philip Stark besprochen und als Best Practice f√ºr Cloud-Native Projekte best√§tigt.

---

### 2.7.2 Gr√ºnde f√ºr den Wechsel

- **Zentrale Plattform:** Code, Doku und Tasks an einem Ort  
- **Nachvollziehbarkeit:** Verkn√ºpfung von Issues, Commits und Deployments  
- **Automatisierung:** Actions k√∂nnen Status oder Nachweise direkt aktualisieren  
- **Klarer Prozess:** Backlog ‚Üí Sprint ‚Üí Review ‚Üí Done (identisch mit Planner, aber nachvollziehbar versioniert)  
- **GitHub Labels und Milestones:** Ersetzen Buckets und F√§lligkeitsdaten aus Planner  
- **Synchronit√§t mit CI/CD:** Automatische Workflows binden den Projektfortschritt direkt an den technischen Build Prozess  

Verwendete Labels:
- `sprint-1`, `sprint-2`, `sprint-3`
- `ci-cd`, `kubernetes`, `documentation`, `review`, `done`

---

### 2.7.3 Fazit

Der Wechsel zu GitHub Projects und Issues hat sich als entscheidender Schritt erwiesen.  
Er erm√∂glicht einen durchg√§ngigen Arbeitsfluss zwischen Code, Automation und Projektmanagement, ohne Tool-Br√ºche.  
Zudem ist die Nachweisf√ºhrung durch Screenshots, Actions-Runs und verlinkte Dokumentation klar pr√ºfbar und Versionen lassen sich √ºber Commit-SHAs direkt nachvollziehen.



## SWOT-Analyse

Die SWOT-Analyse fasst die internen und externen Faktoren des Projekts zusammen und dient zur Bewertung der technologischen Tragf√§higkeit und prozessualen Stabilit√§t. Die SWOT-Analyse bietet einen strukturierten √úberblick √ºber die internen St√§rken und Schw√§chen sowie die externen Chancen und Risiken des Projekts. Ziel ist es, das Projekt im Hinblick auf seine technologische, organisatorische und strategische Tragf√§higkeit zu reflektieren.

![SWOT-Analyse](./screenshots/SWOT.png)

### St√§rken

- **Cloud Native Architektur**
  - Kubernetes erm√∂glicht skalierbaren und stabilen Betrieb
  - Self Healing durch automatische Pod Neustarts

- **Automatisierte CI/CD Pipeline**
  - Build, Push und Deployment laufen vollautomatisch
  - Reduktion von manuellen Fehlern bei Deployments

- **Moderne DevOps Praktiken**
  - Infrastructure as Code Denkweise
  - Versionierte Deployments √ºber GitHub Container Registry

- **Technologie Unabh√§ngigkeit**
  - Kein Vendor Lock In wie bei propriet√§ren Plattformen
  - Offene Standards (Docker, Kubernetes, GitHub Actions)

- **Saubere Dokumentation**
  - Projektmanagement und Technik transparent dokumentiert
  - GitHub Pages erm√∂glicht jederzeitigen Zugriff


### Schw√§chen

- **Erh√∂hte Komplexit√§t**
  - Kubernetes und CI/CD erfordern tiefere Einarbeitung
  - H√∂herer initialer Setup Aufwand

- **Single Node Setup**
  - K3s l√§uft auf einer einzelnen EC2 Instanz
  - Keine echte Hochverf√ºgbarkeit

- **Betriebsverantwortung**
  - Wartung und Updates liegen vollst√§ndig beim Betreiber
  - Monitoring und Alerting nur rudiment√§r umgesetzt

- **Kein Managed Service**
  - Im Vergleich zu EKS mehr manueller Aufwand
  - Sicherheitsupdates m√ºssen selbst geplant werden

### Chancen

- **Skalierbarkeit f√ºr Zukunft**
  - Erweiterung auf Multi Node Cluster m√∂glich
  - Einfache Integration weiterer Microservices

- **√úbertragbarkeit auf reale Projekte**
  - Architektur entspricht modernen Unternehmensstandards
  - Direkter Praxisbezug f√ºr DevOps und Cloud Rollen

- **Automatisierungspotenzial**
  - Erweiterbar um Monitoring, Logging und Alerts
  - GitOps Ansatz sp√§ter m√∂glich

- **Weiterentwicklung der Anwendung**
  - Anbindung weiterer Systeme oder Services
  - Trennung von Frontend und Backend m√∂glich

- **Wiederverwendbarkeit**
  - Pipeline und Kubernetes Manifeste k√∂nnen f√ºr andere Projekte genutzt werden


### Risiken

- **Fehlkonfigurationen**
  - Fehler in Kubernetes Manifests k√∂nnen Service Ausfall verursachen
  - Sicherheitsrelevante Fehlkonfigurationen m√∂glich

- **Kostenrisiken**
  - AWS EC2 Kosten bei l√§ngerem Betrieb
  - Speicher und Traffic Kosten bei Skalierung

- **Know how Abh√§ngigkeit**
  - Betrieb erfordert Kubernetes und Linux Wissen
  - Fehlende Erfahrung kann zu Ausf√§llen f√ºhren

- **Zeitliche √úberforderung**
  - Parallel laufende Module k√∂nnen Zeitdruck erzeugen
  - Debugging von CI/CD kann zeitintensiv sein


### Fazit der SWOT Analyse

Die Cloud Native Umsetzung bietet klare Vorteile in Bezug auf Automatisierung, Skalierbarkeit und Wartbarkeit.  
Die erh√∂hte technische Komplexit√§t und der manuelle Betriebsaufwand stellen jedoch Herausforderungen dar.  
Insgesamt √ºberwiegen die St√§rken und Chancen, insbesondere im Hinblick auf Lerngewinn und Praxisn√§he f√ºr moderne IT Betriebsmodelle.




---

## Use-Case Diagramm

Das Use-Case Diagramm zeigt die Akteure und Interaktionen mit dem Ger√§teausleihe-System aus technischer Sicht.  
Die Zielgruppe sind vor allem Stakeholder, die den Betrieb oder die Integration bewerten m√∂chten.

![Use-Case](./screenshots/use_case_diagramm.png)
<small><em>Abbildung 12: Use-Case Diagramm Ger√§teausleihe Sem4</em></small>

### 2.9.1 Akteure

| Akteur | Rolle | Berechtigung | Hauptfunktionen |
|--------|-------|---------------|-----------------|
| **Developer / Student** | Projektverantwortlicher | Vollzugriff auf Repo und Cluster | Code, Deploy, Doku |
| **Fachdozent** | Reviewer | Leserechte auf Board und Repo | Sprint Reviews, Feedback |
| **AWS System** | Infrastruktur | Cluster Hosting und Networking | CI/CD Zielsystem |
| **GitHub Actions** | Automatisierung | Build, Push, Deploy | Workflows, Status, Rollbacks |
| **Nutzer (PowerApps)** | Externer Konsument | Zugriff auf /healthz und /pdf | Nutzung der API |

---

### 2.9.2 Use-Cases im Detail

| Use-Case | Beschreibung | Akteur | Priorit√§t |
|-----------|--------------|---------|-----------|
| **UC1** | Code√§nderung pushen | Developer | Hoch |
| **UC2** | Workflow ausf√ºhren | GitHub Actions | Hoch |
| **UC3** | Container bauen und pushen | GitHub Actions / GHCR | Hoch |
| **UC4** | Deployment auf K3s durchf√ºhren | GitHub Actions | Hoch |
| **UC5** | Service testen √ºber /healthz | Nutzer / Dozent | Hoch |
| **UC6** | PDF-Endpoint √ºberpr√ºfen | Nutzer / Dozent | Mittel |
| **UC7** | Rollback auf √§lteres Image durchf√ºhren | Developer | Mittel |
| **UC8** | Dokumentation aktualisieren und deployen | GitHub Actions | Mittel |

---

### 2.9.3 Externe System-Integrationen

| System | Beschreibung | Use-Cases | Schnittstelle |
|---------|---------------|------------|----------------|
| **GitHub Actions** | CI/CD-Automatisierung | UC1 ‚Äì UC4, UC8 | Workflow-YAML |
| **GHCR** | Container Registry f√ºr Images | UC3 ‚Äì UC4 | Docker API |
| **AWS EC2** | Hostet den K3s Cluster | UC4 ‚Äì UC7 | SSH / kubectl |
| **K3s Cluster** | Kubernetes Runtime | UC4 ‚Äì UC7 | API Server |
| **PowerApps** | Client Frontend f√ºr Nutzer | UC5 ‚Äì UC6 | HTTP GET |
| **GitHub Pages** | Dokumentationshosting | UC8 | Static Site Deploy |

---

### 2.9.4 Gesch√§ftsregeln und technische Constraints

**Berechtigungen:**
- Nur der Developer darf Deployments ausf√ºhren.
- Dozenten haben Leserechte auf Board, Repo und Doku.
- GitHub Actions arbeitet mit SSH-Key und Secret Authentication.

**Regeln f√ºr Deployments:**
- Nur √Ñnderungen im Ordner `/service` l√∂sen einen Container Build aus.
- Deployments erfolgen nur auf dem Branch `main`.  
- Erfolgreiche Deployments werden in der Doku unter Evidence pro Sprint nachgewiesen.

**Technische Constraints:**
- Cluster: Einzelinstanz K3s auf AWS EC2 t3.medium  
- Registry: GHCR √∂ffentlich lesbar, privat schreibgesch√ºtzt  
- Keine Persistenz im Cluster, da State nicht Teil des Projekts  
- Keine TLS Zertifikate, da nip.io Host f√ºr interne Demo

---

## Risikomatrix

Die Risikomatrix dient zur strukturierten Bewertung potenzieller Risiken im Projekt.

Bewertet werden Risiken aus den Bereichen **Infrastruktur, Kubernetes, CI/CD, Sicherheit und Betrieb**.  
Die Kombination aus Eintrittswahrscheinlichkeit und Auswirkung zeigt die Dringlichkeit notwendiger Gegenmassnahmen.

![Risikomatrix](./screenshots/Risikomatrix_Sem4.png)



### Achsenbeschreibung

- **Y-Achse:** Eintrittswahrscheinlichkeit  
  (Unwahrscheinlich ‚Üí Sehr wahrscheinlich)

- **X-Achse:** Auswirkung  
  (Niedrig ‚Üí Kritisch)

### Farbbedeutung

- üü© Gr√ºn: Geringes Risiko  
- üü® Gelb: Akzeptables Risiko  
- üüß Orange: Erh√∂htes Risiko  
- üî¥ Rot: Kritisches Risiko

---

### 2.10.1 Risiken im Detail

| Nr. | Risiko                                                                 | Eintritt        | Auswirkung | Bewertung | Massnahme / L√∂sung |
|----:|------------------------------------------------------------------------|-----------------|------------|-----------|--------------------|
| 1 | Fehlkonfiguration von Kubernetes Manifests (Pods starten nicht) | Gelegentlich | Hoch | Orange | Manifeste schrittweise testen, `kubectl apply --dry-run`, Logs pr√ºfen |
| 2 | CI Pipeline schl√§gt fehl durch falsche Secrets oder Tokens | Wahrscheinlich | Mittel | Orange | Secrets fr√ºh testen, klare Namenskonventionen, Test Runs |
| 3 | Kein Zugriff auf EC2 Instanz (SSH Key verloren oder Security Group Fehler) | Unwahrscheinlich | Kritisch | Gelb | SSH Keys sichern, Dokumentation der Zug√§nge, Fallback Zugriff |
| 4 | Container Image wird nicht korrekt nach GHCR gepusht | Gelegentlich | Mittel | Gelb | Image Tags pr√ºfen, lowercase Repo Namen erzwingen |
| 5 | Ingress ist falsch konfiguriert, Service nicht erreichbar | Gelegentlich | Hoch | Orange | Ingress separat testen, Logs des Ingress Controllers pr√ºfen |
| 6 | K3s Dienst oder Node f√§llt aus (Single Node Setup) | Selten | Hoch | Gelb | Neustart Strategien dokumentieren, Risiko bewusst akzeptieren |
| 7 | Fehlende Health Checks f√ºhren zu instabilem Betrieb | Wahrscheinlich | Mittel | Orange | Readiness und Liveness Probes definieren |
| 8 | Fehlerhafte CI/CD √Ñnderung deployed fehlerhafte Version | Gelegentlich | Hoch | Orange | Deployment nur √ºber main, saubere Reviews, Rollback via Image Tag |
| 9 | Dokumentation nicht aktuell zum Projektstand | Gelegentlich | Niedrig | Gr√ºn | Doku als Teil der Definition of Done |
| 10 | Zeitmangel durch parallele Module und Aufgaben | Sehr Wahrscheinlich | Hoch | Rot | Priorisierung auf Kernanforderungen, Sprint Planung strikt einhalten |

---

### 2.10.2 Einordnung in die Risikomatrix

Die Risiken wurden in der Risikomatrix wie folgt positioniert:

- **Orange (erh√∂htes Risiko):** 1, 2, 5, 7, 8  
- **Gelb (akzeptables Risiko):** 3, 4, 6  
- **Gr√ºn (geringes Risiko):** 9  
- **Rot (kritisch):** 10


---

### 2.10.3 Fazit

- Insgesamt wurden 10 projektrelevante Risiken identifiziert und bewertet.
- Mehrere Risiken befinden sich im orangefarbenen Bereich, was die erh√∂hte technische Komplexit√§t von Kubernetes und CI/CD widerspiegelt.
- Ein Risiko (Nr. 10: Zeitmangel durch parallele Module und Aufgaben) wurde bewusst als **kritisch (rot)** eingestuft, da die zeitlichen Rahmenbedingungen w√§hrend des Semesters eine reale und hohe Gefahr darstellen.
- Dieses Risiko wird durch klare Priorisierung der Kernanforderungen, Sprint-Planung sowie konsequente Fokussierung auf Mindestanforderungen aktiv adressiert.
- Insgesamt bleibt das Risikoprofil trotz des identifizierten kritischen Risikos kontrollierbar und angemessen f√ºr ein praxisorientiertes Lernprojekt.

# Architektur

Die folgenden Abschnitte erkl√§ren die drei Diagramme inhaltlich und bezogen auf dein aktuelles Setup mit GitHub, GHCR, AWS EC2, K3s und Traefik Ingress. Externe Erreichbarkeit erfolgt √ºber den Host `geraeteausleihe.<EC2_IP>.nip.io`, die wichtigsten Endpoints sind `/healthz` und `/pdf`.

## Deployment Ablauf

Dieses Diagramm zeigt den Ablauf einer √Ñnderung vom Commit bis zum erfolgreichen Rollout im K3s Cluster.

1. Entwickler pusht √Ñnderungen auf den Branch `main` im GitHub Repository.  
2. GitHub triggert den GitHub Actions Workflow aufgrund des Push Events.  
3. GitHub Actions baut ein neues Docker Image aus dem aktuellen Repository Stand. Dabei wird der Service reproduzierbar erstellt, inklusive Abh√§ngigkeiten.  
4. Das erzeugte Image wird in die GitHub Container Registry GHCR gepusht. Als Tag wird ein eindeutiger Wert genutzt, typischerweise die Commit SHA, damit jede Version klar nachvollziehbar ist.  
5. Danach startet der Deploy Schritt. GitHub Actions authentifiziert sich auf die AWS EC2 Instanz, auf der K3s l√§uft. In deinem Setup passiert das √ºblicherweise √ºber SSH zur EC2 Instanz und anschliessend √ºber `kubectl` Befehle im richtigen Cluster Kontext.  
6. Mit `kubectl apply` werden die Kubernetes Manifeste angewendet oder aktualisiert. Damit sind Namespace, Deployment, Service und Ingress definiert.  
7. K3s f√ºhrt ein Rolling Update durch. Neue Pods werden gestartet, Readiness greift, danach werden alte Pods beendet. Der Service bleibt w√§hrend des Updates erreichbar.  
8. Der Workflow pr√ºft den Rollout Status. Bei Erfolg gilt das Deployment als abgeschlossen. Optional kann danach zus√§tzlich ein externer Smoke Test erfolgen, zum Beispiel ein Request auf `https://geraeteausleihe.<EC2_IP>.nip.io/healthz`.

Kernaussage: Jeder Push auf `main` erzeugt eine neue deployte Version. Der Commit SHA Tag in GHCR dient als Nachweis, welche Version gerade produktiv l√§uft und erm√∂glicht ein sauberes Rollback auf eine fr√ºhere Version.

### Sequenzdiagramm


```mermaid
sequenceDiagram
  autonumber
  participant Dev as Developer
  participant GH as GitHub
  participant GA as GitHub Actions
  participant Reg as GHCR
  participant K as K3s on EC2

  Dev->>GH: Push auf main
  GH->>GA: Trigger Workflow
  GA->>GA: Build Docker Image
  GA->>Reg: Push Image Tag commit sha
  GA->>K: Auth und kubectl apply
  K->>K: Rollout Deployment
  K-->>GA: Rollout Status ok
```

#### Flowchart LR Komponenten und Datenfluss von Entwicklung bis Nutzerzugriff

Dieses Diagramm zeigt die Systemlandschaft und den Datenfluss von links nach rechts, also vom lokalen Arbeiten bis zum Aufruf durch Nutzer.

1. Der Entwickler arbeitet lokal am Service, an den Kubernetes Manifesten und an den Workflows.  
2. Der Code wird ins GitHub Repository gepusht. Das Repository ist die zentrale Quelle f√ºr Source Code, `k8s` Manifeste und Workflows unter `.github/workflows`.  
3. Bei einem Push auf `main` startet GitHub Actions.  
4. GitHub Actions baut das Docker Image und pusht es nach GHCR. Dadurch ist das Artefakt zentral verf√ºgbar und eindeutig versioniert.  
5. Anschliessend f√ºhrt GitHub Actions den Deploy Job aus und verbindet sich mit der AWS EC2 Instanz, auf der K3s l√§uft.  
6. In K3s existiert ein Namespace `geraeteausleihe`. Dort laufen Deployment, Pods, Service und Ingress.  
7. Der Ingress wird √ºber Traefik bereitgestellt. Er nimmt externe HTTP Requests an, matched Host und Pfade und leitet den Traffic intern an den Service weiter.  
8. Der Service verteilt den Traffic auf die laufenden Pods. Die Pods beantworten die Requests, zum Beispiel `/healthz` f√ºr den Health Check und `/pdf` f√ºr die PDF Ausgabe.  
9. Der Nutzer greift extern √ºber `geraeteausleihe.<EC2_IP>.nip.io` zu. nip.io l√∂st den Host automatisch auf die EC2 IP auf und ersetzt damit eine klassische DNS Konfiguration.

Kernaussage: Das Diagramm zeigt die saubere Trennung zwischen Artefakt Ebene und Laufzeit Ebene. Das Image entsteht und lebt in GHCR, die Ausf√ºhrung erfolgt in K3s als Pods, gesteuert durch deklarative Manifeste.

```mermaid
flowchart LR
  Dev[Developer Laptop] --> Repo[GitHub Repository]
  Repo -->|push auf main| Actions[GitHub Actions Workflow]

  Actions --> Build[Build Docker Image]
  Build --> GHCR[GitHub Container Registry]

  Actions --> Deploy[Deploy Job]
  Deploy -->|kubectl apply| EC2[AWS EC2 Instance]
  EC2 --> K3s[K3s Cluster]

  K3s --> NS[Namespace]
  NS --> DEP[Deployment]
  DEP --> PODS[Pods]
  NS --> SVC[Service]
  NS --> ING[Ingress]

  Users[User Browser] --> ING
  ING --> SVC
  SVC --> PODS

```

#### Flowchart TB interne Kubernetes Struktur im Cluster

Dieses Diagramm zoomt in den K3s Cluster hinein und zeigt die internen Kubernetes Objekte und deren Rollen.

1. K3s ist die Kubernetes Laufzeitplattform auf deiner AWS EC2 Instanz. Sie √ºbernimmt Scheduling, Rollouts, Self Healing und Cluster Networking.  
2. Im Cluster existiert der Namespace `geraeteausleihe`. Er dient der logischen Trennung, verbessert die √úbersicht und erleichtert die Wartung.  
3. Das Deployment beschreibt, welches Container Image laufen soll, wie viele Replikate gew√ºnscht sind und welche Ports und Einstellungen der Service braucht.  
4. Aus dem Deployment entsteht ein ReplicaSet. Das ReplicaSet stellt sicher, dass die gew√ºnschte Anzahl Pods wirklich l√§uft. F√§llt ein Pod aus, wird automatisch ein neuer erstellt.  
5. Die Pods sind die eigentlichen Instanzen deines Flask Microservice. Dort laufen die Endpoints `/healthz` und `/pdf`.  
6. Der Service vom Typ ClusterIP stellt eine interne stabile Adresse bereit und verteilt Requests an die Pods anhand von Labels.  
7. Der Ingress Controller Traefik nimmt externe Requests entgegen. Er leitet sie anhand von Regeln im Ingress Objekt an den Service weiter.  
8. Der Nutzer spricht nur den Ingress an. Alles danach passiert intern √ºber Service und Pod Networking.

Kernaussage: Extern sichtbar ist der Ingress, intern bleibt die Applikation √ºber Service und Deployment abstrahiert. Das entspricht der erwarteten Kubernetes Struktur f√ºr stabile Deployments und Rolling Updates.

```mermaid
flowchart TB
  subgraph K3s[K3s Cluster]
    subgraph NS[Namespace ger√§teausleihe]
      DEP[Deployment]
      DEP --> RS[ReplicaSet]
      RS --> P1[Pod 1]
      RS --> P2[Pod 2]
      SVC[Service ClusterIP]
    end

    INGCTRL[Ingress Controller]
  end

  Users[User Browser] --> INGCTRL
  INGCTRL --> SVC
  SVC --> P1
  SVC --> P2
```
---

### Zielbild

```mermaid
flowchart TD
  subgraph Clients
    PA[PowerApps Client]
    BR[Browser]
  end

  subgraph GitHub
    REPO[Repository]
    ACT[GitHub Actions]
    GHCR[GitHub Container Registry]
    PAGES[GitHub Pages]
  end

  subgraph AWS
    EC2[AWS EC2 Instanz]
    K3S[K3s Cluster]
    TRF[Traefik Ingress]
    SVC[Flask Microservice\nGer√§teausleihe]
  end

  PA -->|HTTP| TRF
  BR -->|HTTP| TRF
  TRF -->|Service Routing| SVC

  REPO --> ACT
  ACT -->|Build und Push| GHCR
  ACT -->|kubectl apply / set image| K3S
  GHCR -->|pull image| SVC

  REPO -->|MkDocs Build| PAGES
```
---

## Komponenten

| Komponente | Aufgabe | Hinweis |
| --- | --- | --- |
| PowerApps | Konsument des Microservice | Ruft den PDF Endpoint auf |
| Flask Microservice | REST API und PDF Ausgabe | L√§uft containerisiert |
| GitHub Actions | Build, Push, Deploy und Docs | CI CD und GitHub Pages |
| GHCR | Container Images | Versionierung √ºber Tags |
| AWS EC2 | Compute Basis | K3s l√§uft auf der Instanz |
| K3s | Kubernetes Distribution | Single Node Betrieb f√ºr das Projekt |
| Traefik Ingress | HTTP Ingress Controller | Routing auf Service |
| nip io | DNS Convenience | Hostname ohne eigene DNS Zone |

---

## Schnittstellen und Endpunkte

| Endpoint | Methode | Zweck | Erwartung |
| --- | --- | --- | --- |
| `/health` | GET | Health Check | 200 OK |
| `/pdf` | GET | PDF Ausgabe f√ºr PowerApps | PDF Response |

---

## Kubernetes Ressourcen

| Manifest | Ressource | Zweck | Wichtige Punkte |
| --- | --- | --- | --- |
| `k8s/namespace.yaml` | Namespace | Trennung der App Ressourcen | eigener Namespace |
| `k8s/deployment.yaml` | Deployment | Pods und Rolling Update | Image Tag, Probes |
| `k8s/service.yaml` | Service | Interner Zugriff | ClusterIP |
| `k8s/ingress.yaml` | Ingress | Externer Zugriff | Host und Path Routing |

---

## Ingress Host Handling

F√ºr die Erreichbarkeit ohne eigene DNS Zone wird nip io verwendet.

| Element | Beispiel |
| --- | --- |
| Muster | `geraeteausleihe.<EC2_IP>.nip.io` |
| Vorteil | Keine DNS Konfiguration, sofort testbar |
| Nachteil | Nicht geeignet f√ºr produktive Umgebungen |

---

## Security und Betrieb

| Bereich | Umsetzung | Offener Ausbau |
| --- | --- | --- |
| Secrets | GitHub Secrets, EC2 SSH, Registry Zugriff | Rotation und Least Privilege |
| Probes | Readiness und Liveness geplant | Grenzwerte und Fehlerf√§lle testen |
| Logs | Container Logs √ºber kubectl abrufbar | Zentrales Logging optional |
| Rollback | √úber Image Tags m√∂glich | Strategie dokumentieren und √ºben |

# Branching Strategie und Regeln

## Ziel
Die Branching Strategie stellt sicher, dass der **main Branch jederzeit stabil** ist und den Stand f√ºr Demo und Abgabe abbildet. Entwicklung und laufende Arbeiten erfolgen auf **develop** oder auf **feature branches**. Die Dokumentation wird √ºber GitHub Pages aus dem Repository ver√∂ffentlicht.

## Branches

- **main**
  - Stabiler Stand f√ºr Sprint Reviews, Demo und Abgabe
  - √Ñnderungen erfolgen nur √ºber Pull Requests
  - Keine direkten Pushes, sofern Branch Regeln aktiv sind

- **develop**
  - Integrationsbranch f√ºr laufende Entwicklung
  - Feature branches werden zuerst nach develop gemerged
  - develop dient als Sammelpunkt bis zum stabilen Merge nach main

- **feature branches**
  - Kurzlebige branches f√ºr einzelne Themen oder Issues
  - Namensschema Empfehlung:
    - `feature/us08-ec2-setup`
    - `feature/us09-k3s-install`
    - `feature/us20-ci-cd-deploy`
  - Nach Abschluss wird per Pull Request nach develop gemerged

- **gh-pages**
  - Enth√§lt die generierte GitHub Pages Ausgabe
  - Wird ausschliesslich durch den GitHub Actions Workflow aktualisiert
  - Keine manuelle Bearbeitung



## Merge Flow

- **Feature Umsetzung**
  - feature branch wird von develop erstellt
  - Pull Request von feature nach develop
  - Review und Checks, dann Merge

- **Sprint Stand oder Release**
  - Pull Request von develop nach main
  - main wird nur gemerged, wenn der Stand stabil ist

## Regeln und Schutz

- **main**
  - Force Push blockiert
  - Direkte Pushes blockiert, Merge nur √ºber Pull Request
  - Status Checks m√ºssen bestehen, falls Workflows definiert sind

- **develop**
  - Direkte Pushes erlaubt, aber bevorzugt √ºber Pull Requests
  - Force Push blockiert, falls Regeln aktiv sind

- **gh-pages**
  - Branch ist gesch√ºtzt
  - Updates erfolgen nur √ºber den GitHub Actions Workflow
  - Keine manuellen Pushes

## Dokumentation und GitHub Pages
Die GitHub Pages Dokumentation wird aus dem Repository gebaut. Der Workflow wird nur ausgef√ºhrt, wenn relevante Dateien ge√§ndert wurden, zum Beispiel `docs` oder `mkdocs.yml`. Dadurch bleibt die Pages Ausgabe konsistent mit dem Stand auf main.

## Commit Konvention
Konvention: `type(scope): message`

Beispiele:
- **docs(pm):** add sprint 1 review and retrospective
- **docs(arch):** add target architecture overview
- **ci(pages):** enable docs deployment workflow
- **ci(cd):** deploy to k3s on push to main
- **feat(k8s):** add deployment and service manifests
- **fix(ci):** correct ghcr image tag
- **chore:** update dependencies

## Definition of Done f√ºr Branching Doku
- **Dokumentation ist committed und gepusht**
- **Merge Flow ist nachvollziehbar beschrieben**
- **Branch Regeln sind dokumentiert**
- **Issue US05 ist im Board auf Done**

