{"config":{"lang":["de"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Start","text":"<p>Geraeteausleihe Microservice Sem4</p> <p>Cloud Native Deployment auf AWS mit Kubernetes, K3s auf EC2 und GitHub Actions</p> <p>      Hier geht es zur Dokumentation!    </p> <p> </p>"},{"location":"#kurzlinks","title":"Kurzlinks","text":"Bereich Link Repository https://github.com/Cancani/geraeteausleihe-sem4 GitHub Projects Board https://github.com/users/Cancani/projects/3/views/1 GitHub Pages https://cancani.com/geraeteausleihe-sem4 Einreichungsformular Efekan_Einreichungsformular_4.SemesterV2.docx"},{"location":"dokumentation/","title":"Dokumentation","text":""},{"location":"dokumentation/#projektdefinition","title":"Projektdefinition","text":""},{"location":"dokumentation/#einreichungsformular","title":"Einreichungsformular","text":"<p>Das Einreichungsformular kann hier entnommen werden: ITCNE24 Semesterarbeit 4 Einreichungsformular</p>"},{"location":"dokumentation/#projektubersicht","title":"Projekt\u00fcbersicht","text":"Eigenschaft Details Titel Ger\u00e4teausleihe Microservice, Cloud Native Deployment auf AWS Studierender Efekan Demirci Dozenten PRJ Corrado Parisi, CNC Philip Stark Semester 4 Semester HF TBZ, ITCNE24 Zeitraum Oktober 2025 bis Januar 2026 Technologie Stack Python Flask, Docker, K3s auf AWS EC2, GitHub Actions, GHCR, MkDocs Architektur Microservice, Container, Kubernetes Deployment, Ingress Routing Repository https://github.com/Cancani/geraeteausleihe-sem4 GitHub Pages https://cancani.com/geraeteausleihe-sem4/ Project Board https://github.com/users/Cancani/projects/3/views/1 <p>Die bestehende Ger\u00e4teausleihe L\u00f6sung aus der vorherigen Semesterarbeit dient als Ausgangsbasis. In dieser Arbeit steht nicht die fachliche Erweiterung im Vordergrund, sondern die Cloud Native Transformation: Containerisierung, automatisierte CI/CD Pipelines sowie der Betrieb in einem Kubernetes Cluster auf AWS.</p> <p>Key Features</p> <ul> <li>Container Build und Push nach GHCR</li> <li>Automatisiertes Deployment nach K3s auf AWS EC2 via GitHub Actions</li> <li>Ingress Routing \u00fcber Traefik und Hostname \u00fcber nip.io</li> <li>Extern erreichbare Endpoints healthz und pdf</li> <li>Laufende Dokumentation \u00fcber MkDocs und GitHub Pages</li> </ul>"},{"location":"dokumentation/#ausgangsbasis-bestehender-microservice-semesterarbeit-3","title":"Ausgangsbasis: Bestehender Microservice (Semesterarbeit 3)","text":"<p>Als fachliche Grundlage dieser Semesterarbeit dient ein bestehender Ger\u00e4teausleihe Microservice, der im Rahmen der Semesterarbeit 3 entwickelt wurde.</p> <p>Der Microservice stellt eine REST API zur Verf\u00fcgung und wird im Ger\u00e4teausleihprozess als Backend Komponente eingesetzt. Er \u00fcbernimmt die serverseitige Verarbeitung und stellt insbesondere die Endpunkte <code>/healthz</code> zur Verf\u00fcgbarkeitspr\u00fcfung sowie <code>/pdf</code> zur Generierung von Ger\u00e4teausleih Quittungen bereit. Die Nutzung erfolgt unter anderem durch eine PowerApps Applikation sowie \u00fcber direkte HTTP Requests.</p> <p>Technologisch basiert der Service auf Python mit Flask. Die Anwendung ist stateless aufgebaut und eignet sich dadurch f\u00fcr einen containerisierten Betrieb. Funktionale Erweiterungen oder \u00c4nderungen an der Fachlogik sind kein Bestandteil dieser Semesterarbeit.</p> <p>In der vorliegenden Semesterarbeit wird der bestehende Microservice als gegebenes System betrachtet. Der Fokus liegt ausschliesslich auf dessen Cloud Native Transformation. Dazu geh\u00f6ren die Containerisierung, der automatisierte Build und Push in eine Container Registry, das Deployment auf ein Kubernetes Cluster sowie der reproduzierbare Betrieb inklusive CI und CD.</p> <p>Die vollst\u00e4ndige fachliche Beschreibung und urspr\u00fcngliche Implementierung des Microservices ist in der Dokumentation der Semesterarbeit 3 enthalten und wird hier bewusst nicht erneut ausgef\u00fchrt.</p> <p>Hier ist die Verlinkung auf die alte Semesterarbeit und die Implementation des Microservices.</p>"},{"location":"dokumentation/#versionskontrolle-und-arbeitsweise-auf-github","title":"Versionskontrolle und Arbeitsweise auf GitHub","text":"<p>Die Umsetzung der Semesterarbeit erfolgte vollst\u00e4ndig versioniert \u00fcber GitHub. Dabei wurde bewusst ein einfaches und kontrolliertes Branching-Modell gew\u00e4hlt, das auf ein Einzelprojekt mit klaren Qualit\u00e4tskontrollen abgestimmt ist.</p> <p>Die Entwicklung erfolgte nicht direkt auf dem produktiven Branch, sondern \u00fcber einen separaten Arbeitsbranch. \u00c4nderungen wurden ausschliesslich \u00fcber Pull Requests integriert und durch automatisierte Checks abgesichert.</p> <p>Kernelemente der Arbeitsweise:</p> <ul> <li>Trennung zwischen stabilem Stand und laufender Entwicklung</li> <li>Nachvollziehbare \u00c4nderungen \u00fcber Pull Requests</li> <li>Automatische Pr\u00fcfungen vor jedem Merge</li> <li>Klare Verbindung zwischen Code, CI/CD und Projektmanagement</li> </ul> <p>Diese Arbeitsweise stellt sicher, dass der <code>main</code>-Branch jederzeit einen stabilen, demo- und abgabef\u00e4higen Stand enth\u00e4lt.</p>"},{"location":"dokumentation/#ausgangslage-und-problemstellung","title":"Ausgangslage und Problemstellung","text":""},{"location":"dokumentation/#ist-zustand","title":"Ist Zustand","text":"<p>Der Service war zwar technisch lauff\u00e4hig, der Betrieb und das Deployment waren jedoch zu wenig standardisiert und zu wenig automatisiert. </p> <p>Typische Nachteile im Ist Zustand:</p> <ul> <li>Deployment Updates waren fehleranf\u00e4llig und schwer nachvollziehbar</li> <li>Kein konsistentes Release und Tagging Konzept</li> <li>Keine Kubernetes Eigenschaften wie Self Healing und deklarative Deployments</li> <li>Nachweise f\u00fcr Stakeholder fehlten oder wurden erst sp\u00e4t nachgetragen</li> </ul> <p>Ist Workflow</p> <pre><code>flowchart LR\n  A[Lokale \u00c4nderung am Code] --&gt; B[Manueller Build oder uneinheitlicher Build]\n  B --&gt; C[Manuelles Deployment]\n  C --&gt; D[Service l\u00e4uft, Status schwer nachvollziehbar]</code></pre> <p>Abbildung 1: Ist-Workflow mit manuellen Build- und Deployment-Schritten</p>"},{"location":"dokumentation/#soll-zustand","title":"Soll Zustand","text":"<p>Ziel ist ein durchg\u00e4ngig automatisierter Workflow vom Commit bis zum laufenden Pod im Cluster. Jede relevante \u00c4nderung soll ein Image bauen, in die Registry pushen und anschliessend automatisiert im K3s Cluster deployed werden.</p> <p>Soll Workflow</p> <pre><code>flowchart LR\n  subgraph GitHub\n    A[Repository] --&gt; B[GitHub Actions]\n    B --&gt; C[GHCR Image]\n    B --&gt; D[GitHub Pages Doku]\n  end\n\n  subgraph AWS\n    E[AWS EC2] --&gt; F[K3s Cluster]\n    F --&gt; G[Traefik Ingress]\n    G --&gt; H[Microservice Pod]\n  end\n\n  C --&gt; H\n  B --&gt; E</code></pre> <p>Abbildung 2: Soll-Workflow mit automatisiertem CI/CD-Prozess \u00fcber GitHub Actions und AWS</p> <p>Soll Verbesserungen</p> Verbesserung Nutzen Umsetzung Automatisierter Build und Push Reproduzierbar, nachvollziehbar GitHub Actions, GHCR Automatisierter Deploy Schneller und konsistent GitHub Actions, kubectl apply Deklarative Kubernetes Manifeste Standardisiertes Deployment Namespace, Deployment, Service, Ingress"},{"location":"dokumentation/#zielsetzung","title":"Zielsetzung","text":"<p>Die Arbeit hat das Ziel, den bestehenden Microservice in eine Cloud Native Betriebsumgebung zu \u00fcberfuehren. Der Fokus liegt auf CI und CD, Kubernetes Deployment, stabiler Erreichbarkeit \u00fcber Ingress sowie einer laufenden Dokumentation, die den Fortschritt und die Qualit\u00e4t belegt.</p> <p>Die Ziele der Semesterarbeit wurden so definiert, dass deren Erreichung objektiv \u00fcberpr\u00fcfbar ist.</p>"},{"location":"dokumentation/#ziel-1-automatisierte-container-erstellung","title":"Ziel 1: Automatisierte Container Erstellung","text":"<p>Der bestehende Ger\u00e4teausleihe Microservice aus dem 3. Semester wird containerisiert und \u00fcber eine CI Pipeline automatisch gebaut.</p> <p>Erfolgskriterium Nach jedem Commit auf den Main Branch wird automatisch ein lauff\u00e4higes Container Image erstellt und in der GitHub Container Registry ver\u00f6ffentlicht.</p>"},{"location":"dokumentation/#ziel-2-automatisiertes-deployment-auf-kubernetes","title":"Ziel 2: Automatisiertes Deployment auf Kubernetes","text":"<p>Der Microservice wird \u00fcber eine CD Pipeline automatisiert auf einem Kubernetes Cluster bereitgestellt.</p> <p>Erfolgskriterium Nach einem erfolgreichen Image Build wird das Deployment im Cluster automatisch aktualisiert und der Service ist ohne manuelle Eingriffe \u00fcber eine \u00f6ffentliche URL erreichbar.</p>"},{"location":"dokumentation/#ziel-3-nachvollziehbare-und-reproduzierbare-systemarchitektur","title":"Ziel 3: Nachvollziehbare und reproduzierbare Systemarchitektur","text":"<p>Die gesamte Systemarchitektur inklusive CI CD Pipeline, Kubernetes Ressourcen und Netzwerkzugriff wird dokumentiert.</p> <p>Erfolgskriterium Ein technisch versierter Dritter kann das Projekt anhand der Dokumentation nachvollziehen und reproduzieren.</p>"},{"location":"dokumentation/#smart-ziele","title":"SMART Ziele","text":"Ziel Spezifisch Messbar Attraktiv Realistisch Terminiert CI Build und Push Container Image wird gebaut und nach GHCR gepusht Erfolgreiche Actions Runs und sichtbare Tags DevOps Nutzen Mit GitHub Actions umsetzbar Sprint 2 und 3 CD Deploy nach K3s Manifeste werden applied und Image wird aktualisiert Pods laufen, Service extern erreichbar Automatisierung Mit K3s auf EC2 umsetzbar Sprint 2 und 3 Ingress und Endpoints Health und PDF funktionieren extern HTTP 200, PDF Response Demo f\u00e4hig Traefik und nip.io vorhanden Sprint 2"},{"location":"dokumentation/#technologien-und-werkzeuge","title":"Technologien und Werkzeuge","text":"Bereich Technologie Begr\u00fcndung Backend Python Flask Schlankes Framework f\u00fcr Microservices Containerisierung Docker Portabilit\u00e4t und reproduzierbarer Betrieb Orchestrierung K3s Kubernetes Betrieb auf einer EC2 Instanz Cloud Hosting AWS EC2 Realistische Zielumgebung f\u00fcr DevOps Deployment Registry GHCR In GitHub integriert, einfache Distribution CI und CD GitHub Actions Automatisierung von Build, Push und Deploy Ingress Traefik Routing im Cluster und externe Erreichbarkeit Dokumentation MkDocs, GitHub Pages Versionierte und laufend publizierte Doku Projektmanagement GitHub Projects, Issues Backlog, Sch\u00e4tzungen, Priorisierung, DoD"},{"location":"dokumentation/#evaluation-technischer-alternativen","title":"Evaluation technischer Alternativen","text":"<p>F\u00fcr die Umsetzung der Semesterarbeit wurden verschiedene technische Optionen betrachtet und bewusst bewertet.</p> <p>F\u00fcr den Kubernetes Betrieb wurde K3s anstelle eines vollst\u00e4ndigen Kubernetes Setups oder eines Managed Services wie Amazon EKS gew\u00e4hlt. K3s reduziert den Betriebsaufwand erheblich und eignet sich f\u00fcr Single Node Szenarien und Lernumgebungen. Ein Managed Kubernetes Service h\u00e4tte zus\u00e4tzliche Kosten und Komplexit\u00e4t verursacht, ohne einen direkten Mehrwert f\u00fcr die Zielsetzung der Arbeit zu liefern.</p> <p>Beim Ingress wurde Traefik gegen\u00fcber Alternativen wie Nginx gew\u00e4hlt, da Traefik standardm\u00e4ssig in K3s integriert ist und eine einfache Konfiguration bietet. F\u00fcr produktive Umgebungen mit komplexeren Routing Anforderungen w\u00e4re Nginx Ingress oder ein dedizierter Load Balancer eine valide Alternative.</p> <p>F\u00fcr die CI CD Umsetzung fiel die Wahl auf GitHub Actions, da Quellcode, Container Registry und Dokumentation bereits auf GitHub zentralisiert sind. Alternativen wie Jenkins oder GitLab CI h\u00e4tten zus\u00e4tzliche Infrastruktur oder einen Plattformwechsel erfordert und wurden daher nicht weiterverfolgt.</p>"},{"location":"dokumentation/#projektmanagement","title":"Projektmanagement","text":""},{"location":"dokumentation/#projektmethodik","title":"Projektmethodik","text":"<p>Das Projekt folgt einem agilen, scrum\u00e4hnlichen Vorgehen mit iterativer Entwicklung und regelm\u00e4ssigen Review Zyklen. Die Planung und Nachverfolgung erfolgt vollst\u00e4ndig in GitHub.</p> <p>Gew\u00e4hlte Methodik: Sprint basierte Entwicklung</p> <p>Die Entscheidung f\u00fcr ein iteratives Vorgehen basiert auf folgenden Punkten:</p> <ul> <li>Neue technische Themen wie Kubernetes, K3s und CI CD ben\u00f6tigen experimentelles Vorgehen mit kurzen Feedback Schleifen</li> <li>Technische Abh\u00e4ngigkeiten werden oft erst w\u00e4hrend der Umsetzung sichtbar, zum Beispiel Ingress, Host Routing und Secrets Handling</li> <li>Dozenten Feedback kann direkt in die n\u00e4chsten Tasks und in die Dokumentation einfliessen</li> <li>Risiken werden fr\u00fch sichtbar, statt erst am Schluss</li> </ul> <p>Kernprinzipien der angewandten Methodik</p> <ul> <li>Iterative Entwicklung mit funktionsf\u00e4higen Zwischenst\u00e4nden</li> <li>Kontinuierliches Feedback und Anpassung der Priorit\u00e4ten</li> <li>Laufende Nachweisf\u00fchrung, damit der Projektstand jederzeit nachvollziehbar ist</li> <li>Klare Definition of Done pro Ticket inklusive Evidence Anforderungen</li> </ul>"},{"location":"dokumentation/#sprintstruktur-im-detail","title":"Sprintstruktur im Detail","text":"<p>Sprint Planning (Sprintbeginn): - Definition von User Stories mit klaren Akzeptanzkriterien - Aufwandssch\u00e4tzung in Story Points - Festlegung des Sprintziels als ein Satz Outcome und der Deliverables - Sprint Scope im GitHub Project Board zuweisen, Sprint Feld und Milestone setzen</p> <p>Sprint Execution (Durchf\u00fchrung): - Kontinuierliche Arbeit an den definierten User Stories - GitHub Issues f\u00fcr Aufgaben Tracking und Statusupdates - Regelm\u00e4ssige Commits und Pushes, kleine \u00c4nderungen statt grosse Spr\u00fcnge - Ticket Status aktuell halten, WIP Limit in Progress maximal 2</p> <p>Sprint Review (Sprintende): - Abgleich gegen Sprintziel - Review Gespr\u00e4che mit Dozenten zur Qualit\u00e4tssicherung - Bewertung der Zielerreichung und Identifikation von Verbesserungspotenzialen - Evidence Pflicht, Screenshots und Links werden direkt pro Sprint Review dokumentiert</p> <p>Sprint Retrospektive: - Reflexion des Arbeitsprozesses mit dem Starfish Modell - Identifikation von Start Doing, Stop Doing, Keep Doing, More Of, Less Of</p> <p>Vorteile der gew\u00e4hlten Methodik: - Flexibilit\u00e4t, schnelle Anpassung an neue Erkenntnisse, zum Beispiel Ingress Routing oder CI CD Details - Qualit\u00e4tssicherung, regelm\u00e4ssige Reviews verhindern sp\u00e4te Richtungs\u00e4nderungen - Motivation, sichtbare Fortschritte nach jedem Sprint - Lernoptimierung, Retrospektiven f\u00fchren zu kontinuierlicher Prozessverbesserung</p>"},{"location":"dokumentation/#projektphasen-und-meilensteine","title":"Projektphasen und Meilensteine","text":"<p>Das Projekt ist in drei Sprints gegliedert. Die Sprintzeitr\u00e4ume entsprechen der urspr\u00fcnglichen Planung. Inhalte aus Sprint 1 und Sprint 2 wurden in einem sp\u00e4teren Zeitraum konzentriert nachgezogen. Die Dokumentation wird nun strukturiert und evidenzbasiert erg\u00e4nzt, damit der Stand jederzeit nachvollziehbar ist.</p>"},{"location":"dokumentation/#sprint-progression-im-uberblick","title":"Sprint Progression im \u00dcberblick","text":"<p>Sprint 1 Projektbasis: Aufbau der Projektbasis mit Board Struktur, Labels, Milestones, Issue Standards und erstem Architektur Zielbild.</p> <p>Sprint 2 Cluster und Deploy: Aufbau der Laufzeitumgebung auf AWS EC2 mit K3s, Kubernetes Ressourcen und Ingress \u00fcber Traefik. Erste externe Tests der Endpoints.</p> <p>Sprint 3 CI CD und Nachweise: Stabilisierung von Build und Deploy Workflows mit GitHub Actions, saubere Versionierung,  Rollback Vorgehen.</p> <p>Laufende Nachweise pro Sprint durchgehend.</p> <p></p> <p>Abbildung 3: Sprint-\u00dcbersicht im GitHub Project Board</p>"},{"location":"dokumentation/#zeitplan","title":"Zeitplan","text":"Sprint Zeitraum Fokus Status Sprint 1 27.10.2025 bis 17.11.2025 Projektbasis, Board Setup, Standards, Architektur Zielbild Abgeschlossen Sprint 2 18.11.2025 bis 15.12.2025 AWS EC2 und K3s Setup, Kubernetes Manifeste, Ingress, erste externe Tests Abgeschlossen Sprint 3 16.12.2025 bis 21.01.2026 CI CD stabilisieren, Evidence pro Sprint, Dokumentation Finalisierung, Abgabe Vorbereitung In Arbeit"},{"location":"dokumentation/#anpassung-der-sprint-planung","title":"Anpassung der Sprint Planung","text":"<p>In der praktischen Umsetzung wurden zentrale Inhalte aus Sprint 1 und Sprint 2 in einem sp\u00e4teren Zeitraum konzentriert nachgezogen. Gr\u00fcnde daf\u00fcr waren parallele Verpflichtungen und die Priorisierung des technischen Fortschritts vor der finalen Dokumentationsform.</p> <p>Die Sanierung erfolgt durch folgende Massnahmen:</p> <ul> <li>Backlog wird konsequent priorisiert und gesch\u00e4tzt</li> <li>Definition of Done wird strikt eingehalten, closed Tickets werden bei offenen DoD Punkten wieder ge\u00f6ffnet</li> <li>Sprint Reviews enthalten ab jetzt konkrete Nachweise, nicht nur Text</li> <li>Status Updates erfolgen regelm\u00e4ssig, inklusive Links auf Board und Dokumentation</li> </ul>"},{"location":"dokumentation/#issues-und-user-stories","title":"Issues und User Stories","text":"<p>Das Projekt umfasst 27 User Stories, US01 bis US27. Alle Stories werden als GitHub Issues gef\u00fchrt und im GitHub Project Board verwaltet.</p>"},{"location":"dokumentation/#standards-pro-issue","title":"Standards pro Issue","text":"<ul> <li>User Story Text</li> <li>Akzeptanzkriterien als Checkboxen</li> <li>Definition of Done als Checkboxen</li> <li>Labels f\u00fcr Priorit\u00e4t und Bereich</li> <li>Milestone Zuordnung zu Sprint</li> </ul>"},{"location":"dokumentation/#project-board-felder","title":"Project Board Felder","text":"<p>Die Steuerung erfolgt \u00fcber folgende Felder im GitHub Project:</p> Feld Zweck Status Backlog, Ready, In Progress, Review, Done Story Points Aufwandssch\u00e4tzung Priorit\u00e4t Must, Should, Could Sprint Sprint 1, Sprint 2, Sprint 3 DoD erf\u00fcllt Ja, Nein"},{"location":"dokumentation/#board-workflow","title":"Board Workflow","text":"Spalte Bedeutung Backlog Neue Anforderungen, noch nicht priorisiert Ready Priorisiert und bereit zur Umsetzung In Progress Aktive Umsetzung, WIP Limit beachten Review DoD Kontrolle, Evidence pr\u00fcfen Done Abgeschlossen und dokumentiert"},{"location":"dokumentation/#aufwandsschatzung-und-story-points-issues-und-user-stories","title":"Aufwandssch\u00e4tzung und Story Points (Issues und User Stories)","text":"<p>Die Aufw\u00e4nde der einzelnen Issues und User Stories wurden mit Story Points gesch\u00e4tzt. Story Points stellen bewusst keine Zeitangaben dar, sondern dienen als relative Bewertung von Aufwand, Komplexit\u00e4t und Risiko.</p> <p>Die Sch\u00e4tzung basiert auf folgenden Kriterien: - Technische Komplexit\u00e4t der Aufgabe - Anzahl beteiligter Komponenten (Service, CI/CD, Kubernetes, Cloud) - Grad der Unsicherheit oder Neuartigkeit der Technologie - Erwarteter Analyse-, Test- und Debuggingaufwand - Abh\u00e4ngigkeiten zu anderen Tasks</p> <p>Es wurde bewusst auf eine Sch\u00e4tzung in Stunden verzichtet, da diese insbesondere bei technischen Aufgaben mit hohem Lern- und Analyseanteil eine Scheingenauigkeit erzeugen w\u00fcrde.</p>"},{"location":"dokumentation/#priorisierung","title":"Priorisierung","text":"<p>Die Priorisierung der Issues erfolgt zentral im GitHub Project Board und ist unabh\u00e4ngig von einzelnen Sprints. Ziel der Priorisierung ist es, den Fokus auf fachlich und technisch kritische Aufgaben zu legen und Abh\u00e4ngigkeiten fr\u00fchzeitig zu ber\u00fccksichtigen.</p> <p>Die Priorisierung basiert auf folgenden Kriterien: - Technische Abh\u00e4ngigkeiten zu anderen Tasks - Risiko f\u00fcr den Projektfortschritt oder Betrieb - Kritikalit\u00e4t f\u00fcr einen lauff\u00e4higen End to End Betrieb - R\u00fcckmeldungen und Anforderungen der Dozenten</p> <p>Die Priorit\u00e4t wird pro Issue explizit festgelegt und bleibt \u00fcber mehrere Sprints hinweg sichtbar. Dadurch ist jederzeit nachvollziehbar, warum bestimmte Aufgaben fr\u00fcher umgesetzt wurden als andere.</p> <p>Die Priorisierung wird im Project Board visuell dargestellt. Issues mit der Priorit\u00e4t Must sind im Backlog ganz oben angeordnet, darunter folgen Issues mit der Priorit\u00e4t Should, w\u00e4hrend Issues mit der Priorit\u00e4t Could bewusst am unteren Ende des Backlogs platziert sind.</p> <p>Diese Anordnung stellt sicher, dass fachlich und technisch zwingend notwendige Aufgaben jederzeit klar erkennbar sind und zuerst in die Sprint Planung einfliessen.</p> <p></p> <p>Abbildung 4: Priorisierung im GitHub Project Board</p>"},{"location":"dokumentation/#verwendete-story-point-skala","title":"Verwendete Story-Point-Skala","text":"Story Points Bedeutung 1 Sehr kleiner Task, klar abgegrenzt, kaum Risiko 2 Kleiner Task mit \u00fcberschaubarem Aufwand 3 Mittlerer Task mit mehreren Schritten oder Abh\u00e4ngigkeiten 5 Komplexer Task oder neue Technologie mit erh\u00f6htem Debuggingaufwand 8 Sehr komplexer Task mit hohem Risiko oder vielen Unbekannten <p>Die Story Points werden direkt pro Issue im GitHub Project Board gepflegt. Zus\u00e4tzlich ist in jedem Issue eine kurze Begr\u00fcndung der Sch\u00e4tzung dokumentiert. \u00c4nderungen an Sch\u00e4tzungen wurden transparent im jeweiligen Issue festgehalten.</p>"},{"location":"dokumentation/#sprint-planungen-reviews-retrospektiven","title":"Sprint Planungen, Reviews &amp; Retrospektiven","text":"<p>Die nachfolgenden Abschnitte dokumentieren den vollst\u00e4ndigen Projektverlauf und machen Fortschritte, Entscheidungen und Herausforderungen transparent nachvollziehbar.</p>"},{"location":"dokumentation/#sprint-1-planung-review-und-retrospektive","title":"Sprint 1 Planung, Review und Retrospektive","text":""},{"location":"dokumentation/#sprint-zeitraum","title":"Sprint Zeitraum","text":"<p>27.10.2025 bis 17.11.2025</p>"},{"location":"dokumentation/#sprintziel","title":"Sprintziel","text":"<p>Projektbasis schaffen, damit Fortschritt und Qualit\u00e4t transparent nachvollziehbar sind. Fokus liegt auf Board Setup, Standards und erstem Architektur Zielbild.</p> <p></p> <p>Abbildung 5: Sprint 1 Meilensteine und Issues</p>"},{"location":"dokumentation/#sprint-1-user-stories","title":"Sprint 1 User Stories","text":"<p>Die folgenden User Stories geh\u00f6ren zu Sprint 1:</p> <p>Link zu Issues auf GitHub Projects</p> US Titel Bereich Story Points US01 Kanban Board finalisieren Projektmanagement 1 US02 Labels anlegen Projektmanagement 1 US03 Milestones anlegen Projektmanagement 1 US04 Issue Template einrichten Projektmanagement 2 US05 Branching Strategie dokumentieren Dokumentation 2 US06 Sprint 1 Review und Retro dokumentieren Dokumentation 2 US07 Architektur Zielbild skizzieren Architektur 3 <p>Geplanter Aufwand Sprint 1: 12 Story Points</p> <p>WIP Regel</p> <p>In Progress maximal 2 parallel laufende Issues.</p>"},{"location":"dokumentation/#evidence-standard-fur-sprint-1","title":"Evidence Standard f\u00fcr Sprint 1","text":"<p>F\u00fcr Sprint 1 werden mindestens folgende Nachweise geplant:</p> <ul> <li>Screenshot Project Board \u00dcbersicht</li> <li>Screenshot Labels</li> <li>Screenshot Milestones</li> <li>Screenshot Issue Template</li> <li>Link zur Branching Dokumentation</li> <li>Architektur Zielbild als Diagramm</li> </ul>"},{"location":"dokumentation/#sprint-1-review","title":"Sprint 1 Review","text":"<p>Reviewgespr\u00e4ch Hinweis F\u00fcr Sprint 1 fand kein Reviewgespr\u00e4ch statt. R\u00fcckmeldungen und Hinweise wurden schriftlich via Microsoft Teams ausgetauscht.</p>"},{"location":"dokumentation/#review-ergebnis","title":"Review Ergebnis","text":"<p>Sprint 1 wurde umgesetzt. Die Projektbasis ist vorhanden und bildet die Grundlage f\u00fcr Sprint 2 und Sprint 3.</p> Review Punkt Ergebnis Board Struktur vorhanden und nachvollziehbar Erf\u00fcllt Labels vorhanden und konsistent genutzt Erf\u00fcllt Milestones f\u00fcr Sprints vorhanden Erf\u00fcllt Issue Template mit Akzeptanzkriterien und DoD vorhanden Erf\u00fcllt Branching Strategie dokumentiert Erf\u00fcllt Architektur Zielbild skizziert Erf\u00fcllt <p>Umgesetzter Aufwand: 12 von 12 Story Points</p> <p></p> <p>Abbildung 6: Abgeschlossene Tasks in Sprint 1</p>"},{"location":"dokumentation/#board-und-planung","title":"Board und Planung","text":"<p>Project Board Overview  </p> <p></p> <p>Abbildung 7: GitHub Project Board Ansicht f\u00fcr Sprint 1</p> <ul> <li>Labels  </li> </ul> <p></p> <p>Abbildung 8: Issue-Labels zur Kategorisierung</p> <ul> <li>Milestones </li> </ul> <p></p> <p>Abbildung 9: Meilenstein-\u00dcbersicht im Projekt</p> <ul> <li>Issue Template  </li> </ul> <p></p> <p>Abbildung 10: Issue-Template f\u00fcr strukturierte Erfassung</p> <p></p> <p>Abbildung 11: Issue-Template f\u00fcr strukturierte Erfassung</p>"},{"location":"dokumentation/#sprint-1-retrospektive","title":"Sprint 1 Retrospektive","text":"<p>Abbildung 12: Starfish Retrospektive Sprint 1</p> <p>Die Retrospektive wurde mithilfe des Starfish-Modells durchgef\u00fchrt und reflektiert die Erfahrungen des ersten Sprints.</p>"},{"location":"dokumentation/#zusammenfassung","title":"Zusammenfassung","text":"<p>Positiv hervorzuheben ist, dass die Projektbasis sauber aufgebaut wurde und damit eine nachvollziehbare Grundlage f\u00fcr die n\u00e4chsten Sprints entstand. Die Aufgaben wurden konsequent als Issues erfasst, \u00fcber ein Kanban Board gesteuert und durch definierte Standards wie Akzeptanzkriterien und Definition of Done strukturiert. Dadurch war der Fortschritt bereits in Sprint 1 transparent sichtbar.</p> <p>Gleichzeitig zeigte Sprint 1, dass Projektmanagement nicht nur aus der Board Struktur besteht, sondern vor allem aus laufender Evidenz und aktiver Kommunikation. R\u00fcckmeldungen erfolgten in Sprint 1 ausschliesslich schriftlich via Teams und ohne separates Reviewgespr\u00e4ch. Dadurch fehlte zeitweise eine klare gemeinsame Standortbestimmung. Ebenfalls wurde deutlich, dass Nachweise und Dokumentation fr\u00fcher und kontinuierlicher gepflegt werden m\u00fcssen, damit Stakeholder den Projektfortschritt jederzeit pr\u00fcfen k\u00f6nnen.</p> <p>F\u00fcr die kommenden Sprints wurde festgehalten, dass visuelle Hilfsmittel wie Starfish, Risiko Matrix und Architektur Grafiken konsequent eingesetzt werden sollen. Zudem muss Dozentenfeedback nicht nur gelesen, sondern direkt als Issue erfasst, priorisiert und nachvollziehbar umgesetzt werden. Die Sprint Planung soll ausserdem klar definierte Sprint Goals enthalten, welche am Sprintende explizit \u00fcberpr\u00fcft werden.</p>"},{"location":"dokumentation/#empfehlungen-fur-die-nachsten-sprints","title":"Empfehlungen f\u00fcr die n\u00e4chsten Sprints","text":"<ul> <li>Mehr Zeitpuffer beim Sprintbeginn einplanen und Aufgaben fr\u00fch sch\u00e4tzen  </li> <li>Retrospektiven konsequent visuell dokumentieren und konkrete Massnahmen ableiten  </li> <li>Nachweise pro erledigtem Ticket sofort erg\u00e4nzen und nicht erst am Sprintende  </li> <li>Dozentenfeedback direkt als Issue erfassen, priorisieren und mit Belegen abschliessen</li> </ul>"},{"location":"dokumentation/#sprint-2-planung","title":"Sprint 2 Planung","text":""},{"location":"dokumentation/#sprint-zeitraum_1","title":"Sprint Zeitraum","text":"<p>18.11.2025 bis 15.12.2025 (Wurde am 07.01.2026 durch Feedback von Fachdozenten verbessert)</p>"},{"location":"dokumentation/#sprint-ziel","title":"Sprint Ziel","text":"<p>Technische Umsetzung f\u00fcr den Cloud Native Betrieb liefern und gleichzeitig die Nachvollziehbarkeit massiv verbessern. Der Fokus liegt auf einem lauff\u00e4higen End to End Betrieb mit Container, GHCR, K3s, Kubernetes und Ingress sowie auf sauberen Nachweisen, realistischen Sch\u00e4tzungen und einem sichtbar priorisierten Backlog.</p> <p></p> <p>Abbildung 13: Sprint 2 Meilensteine und Issues</p> <p>Reviewgespr\u00e4ch Hinweis F\u00fcr Sprint 2 fand kein Reviewgespr\u00e4ch statt. Feedback erfolgte schriftlich via Microsoft Teams durch Corrado Parisi.</p>"},{"location":"dokumentation/#sprint-2-scope","title":"Sprint 2 Scope","text":"<p>Die folgenden Umsetzungspakete geh\u00f6ren zu Sprint 2:</p> Bereich Inhalt Service Bestehenden Flask Service aufbereiten, Requirements bereinigen und Tests ausf\u00fchrbar machen Containerisierung Dockerfile erstellen sowie Image lokal bauen und testen Registry Build und Push nach GitHub Container Registry (GHCR) mit sinnvollen Tags wie latest und Commit SHA Kubernetes Namespace, Deployment, Service und Ingress Manifeste erstellen Infrastruktur AWS EC2 bereitstellen, K3s installieren und Traefik Ingress nutzen Deployment Manuelles Deployment verifizieren und anschliessend GitHub Actions Deployment auf EC2 umsetzen Nachweise Pro abgeschlossenem Punkt Belege erfassen wie Screenshots, Logs, curl Ausgaben, kubectl Status und Actions Runs PM Verbesserungen Board Sichtbarkeit sicherstellen, Sch\u00e4tzungen und Priorisierung einf\u00fchren sowie DoD in Tickets konsequent schliessen Dokumentation MkDocs Struktur pflegen, Navigation bereinigen und Grafiken erg\u00e4nzen"},{"location":"dokumentation/#sprint-2-user-stories","title":"Sprint 2 User Stories","text":"<p>Die folgenden User Stories geh\u00f6ren zu Sprint 2:</p> <p>Link zu Issues auf GitHub Projects</p> US Titel Bereich Story Points US08 EC2 Instanz vorbereiten Infrastruktur 3 US09 K3s auf EC2 installieren Infrastruktur 5 US10 Namespace und Basis Ressourcen erstellen Kubernetes 2 US11 Dockerfile finalisieren Containerisierung 3 US12 GHCR Push verifizieren Registry 2 US13 Kubernetes Deployment Manifest erstellen Kubernetes 3 US14 Kubernetes Service Manifest erstellen Kubernetes 2 US15 Ingress konfigurieren Kubernetes 5 US16 Readiness und Liveness Probes definieren Kubernetes 3 US17 Sprint 2 Review und Retro dokumentieren Dokumentation 2 <p>Geplanter Aufwand Sprint 2: 30 Story Points</p>"},{"location":"dokumentation/#wip-regel","title":"WIP Regel","text":"<p>In Progress sind maximal zwei parallel laufende Issues erlaubt.</p>"},{"location":"dokumentation/#evidence-standard-fur-sprint-2","title":"Evidence Standard f\u00fcr Sprint 2","text":"<p>F\u00fcr Sprint 2 werden mindestens folgende Nachweise geplant und laufend in die technische Dokumentation erg\u00e4nzt:</p> <ul> <li>Screenshot des GHCR Packages mit Tags latest und Commit SHA  </li> <li>Screenshot oder Link zu GitHub Actions Runs f\u00fcr Container Build und Deployment  </li> <li>Screenshot oder Output von Docker Build und lokalen Tests mit pytest  </li> <li>kubectl get nodes und kubectl get pods -A als Nachweis, dass K3s l\u00e4uft  </li> <li>kubectl -n geraeteausleihe get all und get ingress als Nachweis f\u00fcr ein korrektes Deployment  </li> <li>curl Nachweise extern  </li> <li>/healthz  </li> <li>/pdf mit Content Disposition Header  </li> <li>Screenshots der relevanten Dokumentationsseiten wie Technische Umsetzung, Projektmanagement und Sprint Review  </li> <li>Mindestens eine Grafik f\u00fcr die Architektur und eine Grafik f\u00fcr den CI CD Ablauf  </li> <li>Eine Risiko Matrix wird als Grafik erstellt und eingebunden  </li> </ul>"},{"location":"dokumentation/#sprint-2-review","title":"Sprint 2 Review","text":"<p>Reviewgespr\u00e4ch Hinweis F\u00fcr Sprint 2 fand kein Reviewgespr\u00e4ch statt. Feedback erfolgte schriftlich via Microsoft Teams durch Corrado Parisi.</p>"},{"location":"dokumentation/#review-ergebnis_1","title":"Review Ergebnis","text":"<p>Sprint 2 wurde umgesetzt. Der Microservice l\u00e4uft stabil auf AWS EC2 in einem K3s Cluster und ist extern \u00fcber Traefik Ingress erreichbar. Build und Push nach GHCR funktionieren, sowie ein automatisiertes Deployment per GitHub Actions.</p> Review Punkt Ergebnis Service l\u00e4uft lokal und im Container Erf\u00fcllt Tests sind reproduzierbar im Container ausf\u00fchrbar Erf\u00fcllt Docker Image ist gebaut und lauff\u00e4hig Erf\u00fcllt GHCR enth\u00e4lt Tags latest und Commit SHA Erf\u00fcllt AWS EC2 bereitgestellt und erreichbar Erf\u00fcllt K3s l\u00e4uft und kubectl Zugriff ist m\u00f6glich Erf\u00fcllt Kubernetes Namespace Deployment Service Ingress angewendet Erf\u00fcllt Externer Zugriff \u00fcber nip.io Host funktioniert Erf\u00fcllt Deployment Workflow setzt Image und pr\u00fcft Rollout Erf\u00fcllt Nachweise sind zentral dokumentiert Erf\u00fcllt <p>Umgesetzter Aufwand: 27 von 27 Story Points</p> <p></p> <p>Abbildung 14: Abgeschlossene Tasks in Sprint 2</p>"},{"location":"dokumentation/#dozentenfeedback-und-einbau-in-review","title":"Dozentenfeedback und Einbau in Review","text":"<p>Das schriftliche Feedback von Corrado Parisi wurde im Verlauf von Sprint 2 aktiv verarbeitet und in Dokumentation sowie Projektorganisation integriert. Die folgenden Punkte wurden umgesetzt und sind in der Dokumentation und im Project Board nachvollziehbar.</p> Feedback Punkt Umsetzung in Sprint 2 Risikomatrix fehlt Risikomatrix erstellt, bewertet und in der Dokumentation eingebunden Board Link liefert 404 und Board ist nicht sichtbar Project Link aktualisiert und Sichtbarkeit \u00fcberpr\u00fcft Sprints sind unterschiedlich lang Sprintl\u00e4ngen wurden korrigiert Zu wenig \u00dcberblick \u00fcber Projektstand Massnahmen seit Feedback am Montag, 05.01.2026 umgesetzt, Status und Nachweise werden laufend gepflegt Retro k\u00f6nnte noch mehr Struktur haben Retrospektive visuell verbessert und klarer dokumentiert Sprint Goals besser formulieren Sprint Goals in der Dokumentation pr\u00e4ziser formuliert und am Sprintende \u00fcberpr\u00fcfbar gestaltet Wie sch\u00e4tzt du deine Tasks Story Points pro Issue dokumentiert und Aufwand transparent gemacht Priorisierung im Backlog sichtbar machen Issues nach Priorit\u00e4t im Kanban Board sichtbar gemacht und nach Must Should Could gefiltert"},{"location":"dokumentation/#sprint-2-retrospektive","title":"Sprint 2 Retrospektive","text":"<p>Abbildung 15: Starfish Retrospektive Sprint 2</p> <p>Die Retrospektive wurde mithilfe des Starfish Modells durchgef\u00fchrt und reflektiert die Erfahrungen aus Sprint 2. Zus\u00e4tzlich wurde das schriftliche Dozentenfeedback von Corrado Parisi einbezogen, um konkrete Verbesserungen f\u00fcr Transparenz, Nachvollziehbarkeit und technische Stabilit\u00e4t abzuleiten.</p>"},{"location":"dokumentation/#zusammenfassung_1","title":"Zusammenfassung","text":"<p>Positiv hervorzuheben ist, dass der Cloud Native Betrieb technisch umgesetzt und stabil lauff\u00e4hig gemacht wurde. Der Flask Microservice l\u00e4uft containerisiert, Images werden automatisiert nach GHCR gepusht und der Service ist auf AWS EC2 in einem K3s Cluster \u00fcber Traefik Ingress extern erreichbar. Damit ist der zentrale Sprint 2 Anspruch erf\u00fcllt, einen End to end Betrieb aufzubauen, der \u00fcber reale Requests von aussen verifiziert werden kann. Ebenso wurde die technische Dokumentation deutlich ausgebaut, sodass wichtige Schritte und Entscheide nachvollziehbar sind.</p> <p>Gleichzeitig zeigte Sprint 2 klar, dass technische Umsetzung allein nicht gen\u00fcgt, wenn der Projektstand f\u00fcr Stakeholder nicht jederzeit sichtbar ist. Corrado hat insbesondere fehlende Transparenz kritisiert, zum Beispiel fehlender \u00dcberblick \u00fcber Ziele und Herausforderungen, ein zeitweise nicht zug\u00e4ngliches Board sowie uneinheitliche Sprintl\u00e4ngen. Diese Punkte wurden aufgenommen und korrigiert. Der Board Link wurde aktualisiert, die Sprintl\u00e4ngen wurden konsistenter gestaltet und der \u00dcberblick in der Dokumentation wurde verbessert, indem Status, Ziele und n\u00e4chste Schritte klarer beschrieben wurden. Zus\u00e4tzlich wurde die Retrospektive visuell st\u00e4rker aufbereitet und Sprint Goals wurden pr\u00e4ziser formuliert.</p> <p>Ausserdem wurde deutlich, dass Deploy Automatisierung sehr fehleranf\u00e4llig ist, wenn Variablen und Image Tags nicht deterministisch gesetzt werden. Mehrere Fehlerquellen entstanden durch falsche Repo und SHA \u00dcbernahme und durch unstimmige Workflow Logik. Daraus ergibt sich als wichtiger Lernpunkt, dass CI CD Schritte nicht nur gebaut, sondern auch konsequent mit Rollout Status und externem Health Check verifiziert werden m\u00fcssen. Im selben Zuge wurde das Thema Sch\u00e4tzung verbessert. Tasks wurden mit Story Points dokumentiert und die Priorisierung im Board wurde sichtbar gemacht, um Abh\u00e4ngigkeiten und kritische Aufgaben zuerst umzusetzen.</p> <p>F\u00fcr die kommenden Sprints wurde festgehalten, dass die Kombination aus stabiler Technik und laufender Evidenz der Schl\u00fcssel ist. Dozentenfeedback muss fr\u00fch erfasst, im Backlog sichtbar priorisiert und durch konkrete Anpassungen nachweisbar umgesetzt werden. Der Schwerpunkt liegt in Sprint 3 darauf, Deploy und Betrieb weiter zu stabilisieren, die Dokumentation als zentrale Quelle konsequent weiterzuf\u00fchren und den Projektstatus f\u00fcr Dozenten dauerhaft transparent zu halten.</p>"},{"location":"dokumentation/#sprint-3-planung","title":"Sprint 3 Planung","text":""},{"location":"dokumentation/#sprint-zeitraum_2","title":"Sprint Zeitraum","text":"<p>15.12.2025 bis 21.01.2026 (Terminorientiert wegen Gespr\u00e4ch mit Corrado am 13.01.2026)</p>"},{"location":"dokumentation/#sprint-ziel_1","title":"Sprint Ziel","text":"<p>CI und CD f\u00fcr den Cloud Native Betrieb stabil und nachvollziehbar umsetzen. Fokus liegt auf einem reproduzierbaren Build und Push nach GHCR, einem automatischen Deployment nach K3s auf AWS EC2, klarer Tagging Strategie sowie einer vollst\u00e4ndigen technischen Dokumentation als Basis f\u00fcr das Gespr\u00e4ch mit Corrado.</p> <p></p> <p>Abbildung 16: Sprint 3 Meilensteine und Issues</p> <p>Reviewgespr\u00e4ch Hinweis F\u00fcr Sprint 3 ist ein Gespr\u00e4ch mit Corrado am 13.01.2026 geplant. Ziel ist es, bis dahin einen stabilen Stand inklusive Nachweisen und Dokumentation vorzeigen zu k\u00f6nnen.</p>"},{"location":"dokumentation/#sprint-3-scope","title":"Sprint 3 Scope","text":"<p>Die folgenden Umsetzungspakete geh\u00f6ren zu Sprint 3:</p> Bereich Inhalt CI Build GitHub Actions Build robust machen und nur bei relevanten \u00c4nderungen ausf\u00fchren Registry Push nach GHCR mit konsistenten Tags wie latest und Commit SHA sowie lowercase Repository Secrets Zugriff f\u00fcr GHCR und EC2 sauber \u00fcber GitHub Secrets abbilden und testen CD Deployment Automatisches Deployment nach K3s via Workflow, inkl. apply, set image, rollout status Tagging Tagging Strategie definieren und konsistent in Build und Deploy anwenden Tests und Checks Pipeline Testlauf als Nachweis dokumentieren, inklusive Logs und Outputs Dokumentation Technische Dokumentation zu CI, CD und Betrieb aktualisieren und pr\u00fcfbar machen"},{"location":"dokumentation/#sprint-3-user-stories","title":"Sprint 3 User Stories","text":"<p>Die folgenden User Stories geh\u00f6ren zu Sprint 3:</p> <p>Link zu Issues auf GitHub Projects</p> US Titel Bereich Story Points US18 GitHub Actions Build und Push nach GHCR CI 5 US19 Secrets f\u00fcr GHCR und Cluster Zugriff einrichten Security 3 US20 CD Workflow deployt nach K3s Kubernetes 5 US21 Tagging Strategie dokumentieren und anwenden Repo 3 US22 Pipeline Testlauf dokumentieren Testing 3 US23 Sprint 3 Review und Retro dokumentieren PM 2 <p>Geplanter Aufwand Sprint 3: 21 Story Points</p>"},{"location":"dokumentation/#wip-regel_1","title":"WIP Regel","text":"<p>In Progress sind maximal zwei parallel laufende Issues erlaubt.</p>"},{"location":"dokumentation/#evidence-standard-fur-sprint-3","title":"Evidence Standard f\u00fcr Sprint 3","text":"<p>F\u00fcr Sprint 3 werden mindestens folgende Nachweise geplant und laufend in die technische Dokumentation erg\u00e4nzt:</p> <ul> <li>Screenshot oder Link zu GitHub Actions Run f\u00fcr Build und Push nach GHCR  </li> <li>Screenshot GHCR Package mit latest und Commit SHA Tag  </li> <li>Screenshot oder Link zu Deploy Workflow Run inklusive Logs  </li> <li>Output von kubectl get pods und kubectl rollout status als Rollout Nachweis  </li> <li>Screenshot oder Output der gesetzten Image Version im Deployment  </li> <li>Externer Health Check Nachweis \u00fcber den Ingress Host  </li> <li>http://geraeteausleihe.13.223.28.53.nip.io/healthz  </li> <li>Dokumentierte Tagging Strategie mit Begr\u00fcndung  </li> <li>Dokumentierter Pipeline Testlauf als End to End Nachweis  </li> </ul>"},{"location":"dokumentation/#sprint-3-review","title":"Sprint 3 Review","text":"<p>Sprint Review mit Parisi Corrado 13.01.2026</p> <p>Reviewgespr\u00e4ch Hinweis F\u00fcr Sprint 3 fand ein Reviewgespr\u00e4ch mit Corrado Parisi am 13.01.2026 statt. Im Gespr\u00e4ch wurde der aktuelle Stand der Projektdokumentation und des GitHub Projekts gemeinsam gepr\u00fcft und direkt Feedback dazu gegeben.</p>"},{"location":"dokumentation/#kontext","title":"Kontext","text":"<p>Im Call wurde vor allem der aktuelle Stand der Dokumentation, die GitHub Pages Darstellung sowie das GitHub Project Board besprochen. Corrado hat sich die \u00c4nderungen w\u00e4hrend des Calls angesehen und R\u00fcckmeldungen zur Qualit\u00e4t, Nachvollziehbarkeit und Pr\u00e4sentation des Projektstands gegeben.</p>"},{"location":"dokumentation/#positive-ruckmeldungen","title":"Positive R\u00fcckmeldungen","text":"<ol> <li>Die Dokumentation wirkt deutlich verbessert und \u00fcbersichtlicher als zuvor.</li> <li>Die Navigation wurde adressiert und wirkt jetzt deutlich besser.</li> <li>GitHub Pages ist als zentrale Seite sehr sinnvoll, weil alles an einem Ort ist und kein Wechsel zwischen Tools n\u00f6tig ist.</li> <li>Der Workflow \u00fcber Pull Requests wurde als sauberer Prozess gelobt.</li> <li>Erg\u00e4nzungen wie die Risikomatrix wurden ausdr\u00fccklich positiv bewertet.</li> <li>Insgesamt wurde best\u00e4tigt, dass damit viele Punkte gewonnen werden k\u00f6nnen und dass der Stand nach viel Arbeit aussieht.</li> </ol>"},{"location":"dokumentation/#projektmanagement-und-board","title":"Projektmanagement und Board","text":"<ol> <li>WIP Regel ist vorhanden und sinnvoll, konkret maximal 2 parallele Issues.</li> <li>Sch\u00e4tzungen wurden thematisiert und als sinnvoll bewertet.</li> <li>Fibonacci Sequenz f\u00fcr Story Points wurde als passend best\u00e4tigt, weil es um relative Gr\u00f6ssen und nicht um exakte Zeiten geht.</li> <li>Empfehlung: Priorit\u00e4ten und Sch\u00e4tzungen direkt auf den Karten im GitHub Board sichtbar machen, zum Beispiel \u00fcber Labels, damit es f\u00fcr Dritte sofort ersichtlich ist. Dazu soll ein Screenshot in die Dokumentation.</li> </ol>"},{"location":"dokumentation/#dokumentation-und-nachweise","title":"Dokumentation und Nachweise","text":"<ol> <li>Sprint Review Logik wurde angesprochen: pro Sprint klar zeigen, welche Ziele geplant waren und wie sie erreicht wurden.</li> <li>Wenn ein Ziel nicht erreicht wird, ist das okay. Es soll sauber dokumentiert werden, inklusive was in den n\u00e4chsten Sprint wandert.</li> <li>Hyperlinks als Nachweis sind optional, k\u00f6nnen aber helfen, wenn es sinnvoll ist.</li> </ol>"},{"location":"dokumentation/#risikomatrix","title":"Risikomatrix","text":"<ol> <li>Risikomatrix soll laufend gepflegt werden.</li> <li>Zus\u00e4tzliche Idee: Risiko Entwicklung \u00fcber Zeit visualisieren, damit erkennbar wird, ob Risiken kleiner oder gr\u00f6sser werden. Dazu soll ein Beispiel Screenshot geliefert werden.</li> </ol>"},{"location":"dokumentation/#vorschlage-fur-zusatzliche-verbesserungen","title":"Vorschl\u00e4ge f\u00fcr zus\u00e4tzliche Verbesserungen","text":"<ol> <li>Burndown Chart w\u00e4re ein starker Zusatz, falls GitHub Projects das unterst\u00fctzt oder sich alternativ darstellen l\u00e4sst.</li> <li>Eine kurze Demo als GIF aufnehmen und in die Dokumentation einbauen, damit der Effekt sofort sichtbar ist. Philipp soll dabei explizit markiert werden.</li> </ol>"},{"location":"dokumentation/#inhaltliche-detailkritik","title":"Inhaltliche Detailkritik","text":"<ol> <li>Ein kleiner Punkt im Use Case Bereich: ein Eintrag wie Deployment System wirkt dort eventuell nicht passend platziert und k\u00f6nnte optisch besser umgeordnet werden.</li> </ol>"},{"location":"dokumentation/#review-ergebnis_2","title":"Review Ergebnis","text":"<p>Sprint 3 ist inhaltlich und technisch auf einem stabilen Stand. CI Build und Push nach GHCR sind nachvollziehbar, CD Deployment nach K3s l\u00e4uft und die Nachweise sind zentral \u00fcber GitHub Pages auffindbar. Das Review hat best\u00e4tigt, dass der Projektstand deutlich professioneller wirkt, insbesondere durch die strukturierte Dokumentation, den Pull Request Workflow und die Erg\u00e4nzungen im Projektmanagement.</p> Review Punkt Ergebnis Dokumentation strukturierter und besser navigierbar Erf\u00fcllt GitHub Pages als zentrale Projektseite sinnvoll genutzt Erf\u00fcllt Pull Request Workflow konsequent eingesetzt Erf\u00fcllt WIP Limit und Sch\u00e4tzungen etabliert Erf\u00fcllt Risikomatrix vorhanden und nachvollziehbar Erf\u00fcllt CI Build und Push nach GHCR stabil und nachvollziehbar Erf\u00fcllt CD Deployment nach K3s lauff\u00e4hig und pr\u00fcfbar Erf\u00fcllt Evidence Ablage pro Sprint umgesetzt und ausbauf\u00e4hig Teilweise, laufend <p>Umgesetzter Aufwand: 21 von 21 Story Points</p>"},{"location":"dokumentation/#sprint-3-user-stories-und-status","title":"Sprint 3 User Stories und Status","text":"User Story Titel Status Nachweis US18 GitHub Actions Build und Push nach GHCR Erledigt GitHub Actions Run, GHCR Package Tags US19 Secrets f\u00fcr GHCR und Cluster Zugriff einrichten Erledigt GitHub Secrets gesetzt, Deploy Run mit SSH und SCP US20 CD Workflow deployt nach K3s Erledigt Deploy Run, kubectl Status, Ingress erreichbar US21 Tagging Strategie dokumentieren und anwenden Erledigt Tags latest und Commit SHA, lowercase Repo US22 Pipeline Testlauf dokumentieren Erledigt Logs, kubectl Outputs, externe Requests US23 Sprint 3 Review und Retro dokumentieren Erledigt Dieser Abschnitt, Retro erg\u00e4nzt"},{"location":"dokumentation/#offene-punkte-und-nachste-schritte","title":"Offene Punkte und n\u00e4chste Schritte","text":"<ol> <li>Burndown Chart Darstellung im GitHub Projekt pr\u00fcfen und falls m\u00f6glich erg\u00e4nzen.</li> <li>Optional eine sehr kurze Demo als GIF erstellen und in die Dokumentation einbauen, Philipp dabei markieren.</li> <li>Risiko Entwicklung Visualisierung pr\u00fcfen, Beispiel kommt via Screenshot.</li> <li>Use Case Darstellung optisch pr\u00fcfen und Eintrag Deployment System passend umordnen.</li> </ol>"},{"location":"dokumentation/#nachweise-und-screenshots-fur-sprint-3","title":"Nachweise und Screenshots f\u00fcr Sprint 3","text":"<ol> <li>Screenshot GitHub Actions Run Build und Push</li> <li>Screenshot GitHub Actions Run Deploy</li> <li>Screenshot GHCR Package mit latest und Commit SHA</li> <li>Screenshot kubectl get all im Namespace geraeteausleihe</li> <li>Screenshot kubectl get ingress im Namespace geraeteausleihe</li> <li>Screenshot externer Health Check Request auf /healthz</li> <li>Screenshot GitHub Project Board mit sichtbaren Labels f\u00fcr Priorit\u00e4t und Story Points</li> </ol> <p>Abbildung 17: Sprint 3 Meilensteine und Issues</p>"},{"location":"dokumentation/#sprint-3-retrospektive","title":"Sprint 3 Retrospektive","text":"<p>Die Retrospektive wurde mithilfe des Starfish Modells durchgef\u00fchrt und reflektiert die Erfahrungen aus Sprint 3, inklusive des Feedbacks aus dem Reviewgespr\u00e4ch mit Corrado Parisi.</p> <p></p> <p>Abbildung 18: Starfish Retrospektive Sprint 3</p>"},{"location":"dokumentation/#start-doing","title":"Start Doing","text":"<ol> <li>Kurze visuelle Nachweise erg\u00e4nzen, zum Beispiel eine Mini Demo als GIF.</li> <li>Risiko Entwicklung \u00fcber Zeit als Visualisierung pr\u00fcfen und einbauen, damit Fortschritt bei Risiken sichtbar wird.</li> </ol>"},{"location":"dokumentation/#stop-doing","title":"Stop Doing","text":"<ol> <li>Platzhalter \u00dcberschriften wie Screenshots hinzuf\u00fcgen ohne direkte Umsetzung stehen lassen.</li> <li>Uneinheitliche Begriffe in Board und Doku verwenden, zum Beispiel unterschiedliche Namen f\u00fcr dieselben Artefakte.</li> </ol>"},{"location":"dokumentation/#keep-doing","title":"Keep Doing","text":"<ol> <li>Pull Request Workflow beibehalten, weil er Nachvollziehbarkeit und Qualit\u00e4t erh\u00f6ht.</li> <li>GitHub Pages als zentrale Projektseite beibehalten, weil Dozenten und Stakeholder damit alles an einem Ort pr\u00fcfen k\u00f6nnen.</li> <li>WIP Limit maximal 2 beibehalten, weil es Fokus und Durchsatz verbessert.</li> <li>End to End Verifikation regelm\u00e4ssig durchf\u00fchren, kubectl Status plus externe Requests.</li> </ol>"},{"location":"dokumentation/#more-of","title":"More Of","text":"<ol> <li>Visualisierung und klare Sprint Ziele mit messbaren Ergebnissen, damit Reviews schneller pr\u00fcfbar sind.</li> <li>Projektmanagement Nachweise weiter ausbauen, zum Beispiel Board Screenshot pro Sprint mit sichtbaren Priorit\u00e4ten und Sch\u00e4tzungen.</li> <li>Kleine, reproduzierbare Commands und Outputs als Evidence, statt lange Freitexte.</li> </ol>"},{"location":"dokumentation/#less-of","title":"Less Of","text":"<ol> <li>Manuelle Server Eingriffe ohne dokumentierten Grund und ohne Nachweis.</li> <li>Sp\u00e4te Dokumentations Updates kurz vor Reviews, stattdessen kontinuierlich.</li> </ol>"},{"location":"dokumentation/#fazit","title":"Fazit","text":"<p>Sprint 3 hat best\u00e4tigt, dass die Kombination aus stabiler Technik und sichtbarer Evidence entscheidend ist. Das Review Feedback hat vor allem die starke Verbesserung der Dokumentation und des Workflows best\u00e4tigt. Der wichtigste n\u00e4chste Schritt ist, den Projektstand f\u00fcr Dritte noch schneller erfassbar zu machen, insbesondere \u00fcber sichtbare Priorit\u00e4ten und Sch\u00e4tzungen im Board sowie zus\u00e4tzliche visuelle Evidence.</p>"},{"location":"dokumentation/#verwaltung-der-aufgaben","title":"Verwaltung der Aufgaben","text":"<p>Die Aufgaben wurden vollst\u00e4ndig in GitHub Projects organisiert. Das Board ist nach Sprints gegliedert und orientiert sich an der Kanban-Struktur mit den Spalten Backlog, Ready, In Progress, Review und Done.  </p> <p>Jede Aufgabe ist als GitHub Issue (US01 \u2013 US27) angelegt und enth\u00e4lt: - eine klare User Story, - Akzeptanzkriterien und Definition of Done als Checkboxen, - eine Priorit\u00e4t (Must / Should / Could), - die Zuordnung zum Sprint als Milestone</p> <p></p> <p>Abbildung 19: GitHub Project Board Gesamtansicht</p> <p>Der Fortschritt ist \u00fcber das Board jederzeit nachvollziehbar: Geschlossene Issues wandern automatisch nach \u201eDone\u201c, offene bleiben in \u201eReview\u201c, bis alle DoD-Kriterien erf\u00fcllt und Nachweise in der Dokumentation erg\u00e4nzt sind. Diese Struktur sorgt f\u00fcr durchg\u00e4ngige Transparenz im gesamten Projektmanagementprozess.</p>"},{"location":"dokumentation/#wechsel-von-gitlab-zu-github-projects-und-issues","title":"Wechsel von GitLab zu GitHub Projects und Issues","text":""},{"location":"dokumentation/#hintergrund","title":"Hintergrund","text":"<p>Zu Beginn der Semesterarbeit 4 wurde die Projektplanung zun\u00e4chst provisorisch in GitLab vorbereitet, da das Tool visuell bekannt war und bereits in fr\u00fcheren Arbeiten genutzt wurde. Mit dem Start der technischen Umsetzung erfolgte jedoch der Umstieg auf GitHub Projects, um Code, Doku und Aufgabenverwaltung auf einer Plattform zu b\u00fcndeln.  </p> <p>Durch diesen Wechsel konnten alle User Stories direkt mit Commits, Pull Requests und Actions-Runs verkn\u00fcpft werden. Damit war es erstmals m\u00f6glich, Planung, Automatisierung und Deployment vollst\u00e4ndig integriert zu f\u00fchren.</p>"},{"location":"dokumentation/#grunde-fur-den-wechsel","title":"Gr\u00fcnde f\u00fcr den Wechsel","text":"<ul> <li>Zentrale Plattform: Code, Doku und Tasks an einem Ort  </li> <li>Nachvollziehbarkeit: Verkn\u00fcpfung von Issues, Commits und Deployments  </li> <li>Automatisierung: Actions k\u00f6nnen Status oder Nachweise direkt aktualisieren  </li> <li>Klarer Prozess: Backlog \u2192 Sprint \u2192 Review \u2192 Done (identisch mit Planner, aber nachvollziehbar versioniert)  </li> <li>GitHub Labels und Milestones: Ersetzen Buckets und F\u00e4lligkeitsdaten aus Planner  </li> <li>Synchronit\u00e4t mit CI/CD: Automatische Workflows binden den Projektfortschritt direkt an den technischen Build Prozess  </li> </ul>"},{"location":"dokumentation/#fazit_1","title":"Fazit","text":"<p>Der Wechsel zu GitHub Projects und Issues hat sich als entscheidender Schritt erwiesen. Er erm\u00f6glicht einen durchg\u00e4ngigen Arbeitsfluss zwischen Code, Automation und Projektmanagement, ohne Tool-Br\u00fcche. Zudem ist die Nachweisf\u00fchrung durch Screenshots, Actions-Runs und verlinkte Dokumentation klar pr\u00fcfbar und Versionen lassen sich \u00fcber Commit-SHAs direkt nachvollziehen.</p>"},{"location":"dokumentation/#swot-analyse","title":"SWOT-Analyse","text":"<p>Die SWOT-Analyse fasst die internen und externen Faktoren des Projekts zusammen und dient zur Bewertung der technologischen Tragf\u00e4higkeit und prozessualen Stabilit\u00e4t. Die SWOT-Analyse bietet einen strukturierten \u00dcberblick \u00fcber die internen St\u00e4rken und Schw\u00e4chen sowie die externen Chancen und Risiken des Projekts. Ziel ist es, das Projekt im Hinblick auf seine technologische, organisatorische und strategische Tragf\u00e4higkeit zu reflektieren.</p> <p></p> <p>Abbildung 20: SWOT-Analyse f\u00fcr Projektstrategie</p>"},{"location":"dokumentation/#starken","title":"St\u00e4rken","text":"<ul> <li>Cloud Native Architektur</li> <li>Kubernetes erm\u00f6glicht skalierbaren und stabilen Betrieb</li> <li> <p>Self Healing durch automatische Pod Neustarts</p> </li> <li> <p>Automatisierte CI/CD Pipeline</p> </li> <li>Build, Push und Deployment laufen vollautomatisch</li> <li> <p>Reduktion von manuellen Fehlern bei Deployments</p> </li> <li> <p>Moderne DevOps Praktiken</p> </li> <li>Infrastructure as Code Denkweise</li> <li> <p>Versionierte Deployments \u00fcber GitHub Container Registry</p> </li> <li> <p>Technologie Unabh\u00e4ngigkeit</p> </li> <li>Kein Vendor Lock In wie bei propriet\u00e4ren Plattformen</li> <li> <p>Offene Standards (Docker, Kubernetes, GitHub Actions)</p> </li> <li> <p>Saubere Dokumentation</p> </li> <li>Projektmanagement und Technik transparent dokumentiert</li> <li>GitHub Pages erm\u00f6glicht jederzeitigen Zugriff</li> </ul>"},{"location":"dokumentation/#schwachen","title":"Schw\u00e4chen","text":"<ul> <li>Erh\u00f6hte Komplexit\u00e4t</li> <li>Kubernetes und CI/CD erfordern tiefere Einarbeitung</li> <li> <p>H\u00f6herer initialer Setup Aufwand</p> </li> <li> <p>Single Node Setup</p> </li> <li>K3s l\u00e4uft auf einer einzelnen EC2 Instanz</li> <li> <p>Keine echte Hochverf\u00fcgbarkeit</p> </li> <li> <p>Betriebsverantwortung</p> </li> <li>Wartung und Updates liegen vollst\u00e4ndig beim Betreiber</li> <li> <p>Monitoring und Alerting nur rudiment\u00e4r umgesetzt</p> </li> <li> <p>Kein Managed Service</p> </li> <li>Im Vergleich zu EKS mehr manueller Aufwand</li> <li>Sicherheitsupdates m\u00fcssen selbst geplant werden</li> </ul>"},{"location":"dokumentation/#chancen","title":"Chancen","text":"<ul> <li>Skalierbarkeit f\u00fcr Zukunft</li> <li>Erweiterung auf Multi Node Cluster m\u00f6glich</li> <li> <p>Einfache Integration weiterer Microservices</p> </li> <li> <p>\u00dcbertragbarkeit auf reale Projekte</p> </li> <li>Architektur entspricht modernen Unternehmensstandards</li> <li> <p>Direkter Praxisbezug f\u00fcr DevOps und Cloud Rollen</p> </li> <li> <p>Automatisierungspotenzial</p> </li> <li>Erweiterbar um Monitoring, Logging und Alerts</li> <li> <p>GitOps Ansatz sp\u00e4ter m\u00f6glich</p> </li> <li> <p>Weiterentwicklung der Anwendung</p> </li> <li>Anbindung weiterer Systeme oder Services</li> <li> <p>Trennung von Frontend und Backend m\u00f6glich</p> </li> <li> <p>Wiederverwendbarkeit</p> </li> <li>Pipeline und Kubernetes Manifeste k\u00f6nnen f\u00fcr andere Projekte genutzt werden</li> </ul>"},{"location":"dokumentation/#risiken","title":"Risiken","text":"<ul> <li>Fehlkonfigurationen</li> <li>Fehler in Kubernetes Manifests k\u00f6nnen Service Ausfall verursachen</li> <li> <p>Sicherheitsrelevante Fehlkonfigurationen m\u00f6glich</p> </li> <li> <p>Kostenrisiken</p> </li> <li>AWS EC2 Kosten bei l\u00e4ngerem Betrieb</li> <li> <p>Speicher und Traffic Kosten bei Skalierung</p> </li> <li> <p>Know how Abh\u00e4ngigkeit</p> </li> <li>Betrieb erfordert Kubernetes und Linux Wissen</li> <li> <p>Fehlende Erfahrung kann zu Ausf\u00e4llen f\u00fchren</p> </li> <li> <p>Zeitliche \u00dcberforderung</p> </li> <li>Parallel laufende Module k\u00f6nnen Zeitdruck erzeugen</li> <li>Debugging von CI/CD kann zeitintensiv sein</li> </ul>"},{"location":"dokumentation/#fazit-der-swot-analyse","title":"Fazit der SWOT Analyse","text":"<p>Die Cloud Native Umsetzung bietet klare Vorteile in Bezug auf Automatisierung, Skalierbarkeit und Wartbarkeit. Die erh\u00f6hte technische Komplexit\u00e4t und der manuelle Betriebsaufwand stellen jedoch Herausforderungen dar. Insgesamt \u00fcberwiegen die St\u00e4rken und Chancen, insbesondere im Hinblick auf Lerngewinn und Praxisn\u00e4he f\u00fcr moderne IT Betriebsmodelle.</p>"},{"location":"dokumentation/#use-case-diagramm","title":"Use-Case Diagramm","text":"<p>Das Use-Case Diagramm zeigt die Akteure und Interaktionen mit dem Ger\u00e4teausleihe-System aus technischer Sicht. Die Zielgruppe sind vor allem Stakeholder, die den Betrieb oder die Integration bewerten m\u00f6chten.</p> <p></p> <p>Abbildung 21: Use-Case-Diagramm der Systeminteraktionen</p>"},{"location":"dokumentation/#akteure","title":"Akteure","text":"Akteur Rolle Berechtigung Hauptfunktionen Developer / Student Projektverantwortlicher Vollzugriff auf Repo und Cluster Code, Deploy, Doku Fachdozent Reviewer Leserechte auf Board und Repo Sprint Reviews, Feedback AWS System Infrastruktur Cluster Hosting und Networking CI/CD Zielsystem GitHub Actions Automatisierung Build, Push, Deploy Workflows, Status, Rollbacks Nutzer (PowerApps) Externer Konsument Zugriff auf /healthz und /pdf Nutzung der API"},{"location":"dokumentation/#use-cases-im-detail","title":"Use-Cases im Detail","text":"Use-Case Beschreibung Akteur Priorit\u00e4t UC1 Code\u00e4nderung pushen Developer Hoch UC2 Workflow ausf\u00fchren GitHub Actions Hoch UC3 Container bauen und pushen GitHub Actions / GHCR Hoch UC4 Deployment auf K3s durchf\u00fchren GitHub Actions Hoch UC5 Service testen \u00fcber /healthz Nutzer / Dozent Hoch UC6 PDF-Endpoint \u00fcberpr\u00fcfen Nutzer / Dozent Mittel UC7 Rollback auf \u00e4lteres Image durchf\u00fchren Developer Mittel UC8 Dokumentation aktualisieren und deployen GitHub Actions Mittel"},{"location":"dokumentation/#externe-system-integrationen","title":"Externe System-Integrationen","text":"System Beschreibung Use-Cases Schnittstelle GitHub Actions CI/CD-Automatisierung UC1 \u2013 UC4, UC8 Workflow-YAML GHCR Container Registry f\u00fcr Images UC3 \u2013 UC4 Docker API AWS EC2 Hostet den K3s Cluster UC4 \u2013 UC7 SSH / kubectl K3s Cluster Kubernetes Runtime UC4 \u2013 UC7 API Server PowerApps Client Frontend f\u00fcr Nutzer UC5 \u2013 UC6 HTTP GET GitHub Pages Dokumentationshosting UC8 Static Site Deploy"},{"location":"dokumentation/#geschaftsregeln-und-technische-constraints","title":"Gesch\u00e4ftsregeln und technische Constraints","text":"<p>Berechtigungen: - Nur der Developer darf Deployments ausf\u00fchren. - Dozenten haben Leserechte auf Board, Repo und Doku. - GitHub Actions arbeitet mit SSH-Key und Secret Authentication.</p> <p>Regeln f\u00fcr Deployments: - Nur \u00c4nderungen am Build Kontext l\u00f6sen einen Container Build aus. Dazu geh\u00f6ren <code>service/</code>, <code>Dockerfile</code> und Dependency Dateien im Service Kontext. - Deployments erfolgen nur auf dem Branch <code>main</code>. - Erfolgreiche Deployments werden in der Doku unter Evidence pro Sprint nachgewiesen.</p> <p>Technische Constraints: - Cluster: Einzelinstanz K3s auf AWS EC2 t3.medium - Registry: GHCR \u00f6ffentlich lesbar, privat schreibgesch\u00fctzt - Keine Persistenz im Cluster, da State nicht Teil des Projekts - Keine TLS Zertifikate, da nip.io Host f\u00fcr interne Demo</p>"},{"location":"dokumentation/#risikomatrix_1","title":"Risikomatrix","text":"<p>Die Risikomatrix dient zur strukturierten Bewertung potenzieller Risiken im Projekt.</p> <p>Bewertet werden Risiken aus den Bereichen Infrastruktur, Kubernetes, CI/CD, Sicherheit und Betrieb. Die Kombination aus Eintrittswahrscheinlichkeit und Auswirkung zeigt die Dringlichkeit notwendiger Gegenmassnahmen.</p> <p>(Stand 07.01.2026)</p> <p></p> <p>Abbildung 22: Risikomatrix mit identifizierten Projektrisiken</p>"},{"location":"dokumentation/#achsenbeschreibung","title":"Achsenbeschreibung","text":"<ul> <li> <p>Y-Achse: Eintrittswahrscheinlichkeit   (Unwahrscheinlich \u2192 Sehr wahrscheinlich)</p> </li> <li> <p>X-Achse: Auswirkung   (Niedrig \u2192 Kritisch)</p> </li> </ul>"},{"location":"dokumentation/#farbbedeutung","title":"Farbbedeutung","text":"<ul> <li>\ud83d\udfe9 Gr\u00fcn: Geringes Risiko  </li> <li>\ud83d\udfe8 Gelb: Akzeptables Risiko  </li> <li>\ud83d\udfe7 Orange: Erh\u00f6htes Risiko  </li> <li>\ud83d\udd34 Rot: Kritisches Risiko</li> </ul>"},{"location":"dokumentation/#risiken-im-detail","title":"Risiken im Detail","text":"Nr. Risiko Eintritt Auswirkung Bewertung Massnahme / L\u00f6sung 1 Fehlkonfiguration von Kubernetes Manifests (Pods starten nicht) Gelegentlich Hoch Orange Manifeste schrittweise testen, <code>kubectl apply --dry-run</code>, Logs pr\u00fcfen 2 CI Pipeline schl\u00e4gt fehl durch falsche Secrets oder Tokens Wahrscheinlich Mittel Orange Secrets fr\u00fch testen, klare Namenskonventionen, Test Runs 3 Kein Zugriff auf EC2 Instanz (SSH Key verloren oder Security Group Fehler) Unwahrscheinlich Kritisch Gelb SSH Keys sichern, Dokumentation der Zug\u00e4nge, Fallback Zugriff 4 Container Image wird nicht korrekt nach GHCR gepusht Gelegentlich Mittel Gelb Image Tags pr\u00fcfen, lowercase Repo Namen erzwingen 5 Ingress ist falsch konfiguriert, Service nicht erreichbar Gelegentlich Hoch Orange Ingress separat testen, Logs des Ingress Controllers pr\u00fcfen 6 K3s Dienst oder Node f\u00e4llt aus (Single Node Setup) Selten Hoch Gelb Neustart Strategien dokumentieren, Risiko bewusst akzeptieren 7 Fehlende Health Checks f\u00fchren zu instabilem Betrieb Wahrscheinlich Mittel Orange Readiness und Liveness Probes definieren 8 Fehlerhafte CI/CD \u00c4nderung deployed fehlerhafte Version Gelegentlich Hoch Orange Deployment nur \u00fcber main, saubere Reviews, Rollback via Image Tag 9 Dokumentation nicht aktuell zum Projektstand Gelegentlich Niedrig Gr\u00fcn Doku als Teil der Definition of Done 10 Zeitmangel durch parallele Module und Aufgaben Sehr Wahrscheinlich Hoch Rot Priorisierung auf Kernanforderungen, Sprint Planung strikt einhalten"},{"location":"dokumentation/#einordnung-in-die-risikomatrix","title":"Einordnung in die Risikomatrix","text":"<p>Die Risiken wurden in der Risikomatrix wie folgt positioniert:</p> <ul> <li>Orange (erh\u00f6htes Risiko): 1, 2, 5, 7, 8  </li> <li>Gelb (akzeptables Risiko): 3, 4, 6  </li> <li>Gr\u00fcn (geringes Risiko): 9  </li> <li>Rot (kritisch): 10</li> </ul>"},{"location":"dokumentation/#fazit_2","title":"Fazit","text":"<ul> <li>Insgesamt wurden 10 projektrelevante Risiken identifiziert und bewertet.</li> <li>Mehrere Risiken befinden sich im orangefarbenen Bereich, was die erh\u00f6hte technische Komplexit\u00e4t von Kubernetes und CI/CD widerspiegelt.</li> <li>Ein Risiko (Nr. 10: Zeitmangel durch parallele Module und Aufgaben) wurde bewusst als kritisch (rot) eingestuft, da die zeitlichen Rahmenbedingungen w\u00e4hrend des Semesters eine reale und hohe Gefahr darstellen.</li> <li>Dieses Risiko wird durch klare Priorisierung der Kernanforderungen, Sprint-Planung sowie konsequente Fokussierung auf Mindestanforderungen aktiv adressiert.</li> <li>Insgesamt bleibt das Risikoprofil trotz des identifizierten kritischen Risikos kontrollierbar und angemessen f\u00fcr ein praxisorientiertes Lernprojekt.</li> </ul>"},{"location":"dokumentation/#architektur","title":"Architektur","text":"<p>Die folgenden Abschnitte erkl\u00e4ren die drei Diagramme inhaltlich und bezogen auf dein aktuelles Setup mit GitHub, GHCR, AWS EC2, K3s und Traefik Ingress. Externe Erreichbarkeit erfolgt \u00fcber den Host <code>geraeteausleihe.&lt;EC2_IP&gt;.nip.io</code>, die wichtigsten Endpoints sind <code>/healthz</code> und <code>/pdf</code>.</p>"},{"location":"dokumentation/#deployment-ablauf","title":"Deployment Ablauf","text":"<p>Dieses Diagramm zeigt den Ablauf einer \u00c4nderung vom Commit bis zum erfolgreichen Rollout im K3s Cluster.</p> <ol> <li>Entwickler pusht \u00c4nderungen auf den Branch <code>main</code> im GitHub Repository.  </li> <li>GitHub triggert den GitHub Actions Workflow aufgrund des Push Events.  </li> <li>GitHub Actions baut ein neues Docker Image aus dem aktuellen Repository Stand. Dabei wird der Service reproduzierbar erstellt, inklusive Abh\u00e4ngigkeiten.  </li> <li>Das erzeugte Image wird in die GitHub Container Registry GHCR gepusht. Als Tag wird ein eindeutiger Wert genutzt, typischerweise die Commit SHA, damit jede Version klar nachvollziehbar ist.  </li> <li>Danach startet der Deploy Schritt. GitHub Actions authentifiziert sich auf die AWS EC2 Instanz, auf der K3s l\u00e4uft. In deinem Setup passiert das \u00fcblicherweise \u00fcber SSH zur EC2 Instanz und anschliessend \u00fcber <code>kubectl</code> Befehle im richtigen Cluster Kontext.  </li> <li>Mit <code>kubectl apply</code> werden die Kubernetes Manifeste angewendet oder aktualisiert. Damit sind Namespace, Deployment, Service und Ingress definiert.  </li> <li>K3s f\u00fchrt ein Rolling Update durch. Neue Pods werden gestartet, Readiness greift, danach werden alte Pods beendet. Der Service bleibt w\u00e4hrend des Updates erreichbar.  </li> <li>Der Workflow pr\u00fcft den Rollout Status. Bei Erfolg gilt das Deployment als abgeschlossen. Optional kann danach zus\u00e4tzlich ein externer Smoke Test erfolgen, zum Beispiel ein Request auf <code>http://geraeteausleihe.&lt;EC2_IP&gt;.nip.io/healthz</code>.</li> </ol> <p>Kernaussage: Jeder Push auf <code>main</code> erzeugt eine neue deployte Version. Der Commit SHA Tag in GHCR dient als Nachweis, welche Version gerade produktiv l\u00e4uft und erm\u00f6glicht ein sauberes Rollback auf eine fr\u00fchere Version.</p> <p>Sequenzdiagramm</p> <pre><code>sequenceDiagram\n  autonumber\n  participant Dev as Developer\n  participant GH as GitHub\n  participant GA as GitHub Actions\n  participant Reg as GHCR\n  participant K as K3s on EC2\n\n  Dev-&gt;&gt;GH: Push auf main\n  GH-&gt;&gt;GA: Trigger Workflow\n  GA-&gt;&gt;GA: Build Docker Image\n  GA-&gt;&gt;Reg: Push Image Tag commit sha\n  GA-&gt;&gt;K: Auth und kubectl apply\n  K-&gt;&gt;K: Rollout Deployment\n  K--&gt;&gt;GA: Rollout Status ok</code></pre> <p>Abbildung 23: Sequenzdiagramm des Deployment Ablaufs</p>"},{"location":"dokumentation/#flowchart-lr-komponenten-und-datenfluss-von-entwicklung-bis-nutzerzugriff","title":"Flowchart LR Komponenten und Datenfluss von Entwicklung bis Nutzerzugriff","text":"<p>Dieses Diagramm zeigt die Systemlandschaft und den Datenfluss von links nach rechts, also vom lokalen Arbeiten bis zum Aufruf durch Nutzer.</p> <ol> <li>Der Entwickler arbeitet lokal am Service, an den Kubernetes Manifesten und an den Workflows.  </li> <li>Der Code wird ins GitHub Repository gepusht. Das Repository ist die zentrale Quelle f\u00fcr Source Code, <code>k3s/</code> Manifeste und Workflows unter <code>.github/workflows</code>.  </li> <li>Bei einem Push auf <code>main</code> startet GitHub Actions.  </li> <li>GitHub Actions baut das Docker Image und pusht es nach GHCR. Dadurch ist das Artefakt zentral verf\u00fcgbar und eindeutig versioniert.  </li> <li>Anschliessend f\u00fchrt GitHub Actions den Deploy Job aus und verbindet sich mit der AWS EC2 Instanz, auf der K3s l\u00e4uft.  </li> <li>In K3s existiert ein Namespace <code>geraeteausleihe</code>. Dort laufen Deployment, Pods, Service und Ingress.  </li> <li>Der Ingress wird \u00fcber Traefik bereitgestellt. Er nimmt externe HTTP Requests an, matched Host und Pfade und leitet den Traffic intern an den Service weiter.  </li> <li>Der Service verteilt den Traffic auf die laufenden Pods. Die Pods beantworten die Requests, zum Beispiel <code>/healthz</code> f\u00fcr den Health Check und <code>/pdf</code> f\u00fcr die PDF Ausgabe.  </li> <li>Der Nutzer greift extern \u00fcber <code>geraeteausleihe.&lt;EC2_IP&gt;.nip.io</code> zu. nip.io l\u00f6st den Host automatisch auf die EC2 IP auf und ersetzt damit eine klassische DNS Konfiguration.</li> </ol> <p>Kernaussage: Das Diagramm zeigt die saubere Trennung zwischen Artefakt Ebene und Laufzeit Ebene. Das Image entsteht und lebt in GHCR, die Ausf\u00fchrung erfolgt in K3s als Pods, gesteuert durch deklarative Manifeste.</p> <pre><code>flowchart LR\n  Dev[Developer Laptop] --&gt; Repo[GitHub Repository]\n  Repo --&gt;|push auf main| Actions[GitHub Actions Workflow]\n\n  Actions --&gt; Build[Build Docker Image]\n  Build --&gt; GHCR[GitHub Container Registry]\n\n  Actions --&gt; Deploy[Deploy Job]\n  Deploy --&gt;|kubectl apply| EC2[AWS EC2 Instance]\n  EC2 --&gt; K3s[K3s Cluster]\n\n  K3s --&gt; NS[Namespace]\n  NS --&gt; DEP[Deployment]\n  DEP --&gt; PODS[Pods]\n  NS --&gt; SVC[Service]\n  NS --&gt; ING[Ingress]\n\n  Users[User Browser] --&gt; ING\n  ING --&gt; SVC\n  SVC --&gt; PODS\n</code></pre> <p>Abbildung 24: Soll-Workflow mit automatisiertem CI/CD-Prozess \u00fcber GitHub Actions und AWS</p>"},{"location":"dokumentation/#flowchart-tb-interne-kubernetes-struktur-im-cluster","title":"Flowchart TB interne Kubernetes Struktur im Cluster","text":"<p>Dieses Diagramm zoomt in den K3s Cluster hinein und zeigt die internen Kubernetes Objekte und deren Rollen.</p> <ol> <li>K3s ist die Kubernetes Laufzeitplattform auf deiner AWS EC2 Instanz. Sie \u00fcbernimmt Scheduling, Rollouts, Self Healing und Cluster Networking.  </li> <li>Im Cluster existiert der Namespace <code>geraeteausleihe</code>. Er dient der logischen Trennung, verbessert die \u00dcbersicht und erleichtert die Wartung.  </li> <li>Das Deployment beschreibt, welches Container Image laufen soll, wie viele Replikate gew\u00fcnscht sind und welche Ports und Einstellungen der Service braucht.  </li> <li>Aus dem Deployment entsteht ein ReplicaSet. Das ReplicaSet stellt sicher, dass die gew\u00fcnschte Anzahl Pods wirklich l\u00e4uft. F\u00e4llt ein Pod aus, wird automatisch ein neuer erstellt.  </li> <li>Die Pods sind die eigentlichen Instanzen deines Flask Microservice. Dort laufen die Endpoints <code>/healthz</code> und <code>/pdf</code>.  </li> <li>Der Service vom Typ ClusterIP stellt eine interne stabile Adresse bereit und verteilt Requests an die Pods anhand von Labels.  </li> <li>Der Ingress Controller Traefik nimmt externe Requests entgegen. Er leitet sie anhand von Regeln im Ingress Objekt an den Service weiter.  </li> <li>Der Nutzer spricht nur den Ingress an. Alles danach passiert intern \u00fcber Service und Pod Networking.</li> </ol> <p>Kernaussage: Extern sichtbar ist der Ingress, intern bleibt die Applikation \u00fcber Service und Deployment abstrahiert. Das entspricht der erwarteten Kubernetes Struktur f\u00fcr stabile Deployments und Rolling Updates.</p> <pre><code>flowchart TB\n  subgraph K3s[K3s Cluster]\n    subgraph NS[Namespace geraeteausleihe]\n      DEP[Deployment]\n      DEP --&gt; RS[ReplicaSet]\n      RS --&gt; P1[Pod 1]\n      RS --&gt; P2[Pod 2]\n      SVC[Service ClusterIP]\n    end\n\n    INGCTRL[Ingress Controller]\n  end\n\n  Users[User Browser] --&gt; INGCTRL\n  INGCTRL --&gt; SVC\n  SVC --&gt; P1\n  SVC --&gt; P2</code></pre> <p>Abbildung 25: Prozessdiagramm</p>"},{"location":"dokumentation/#ingress-host-handling","title":"Ingress Host Handling","text":"<p>F\u00fcr die Erreichbarkeit ohne eigene DNS Zone wird nip.io verwendet.</p> Element Beispiel Muster <code>geraeteausleihe.&lt;EC2_IP&gt;.nip.io</code> Vorteil Keine DNS Konfiguration, sofort testbar Nachteil Nicht geeignet f\u00fcr produktive Umgebungen"},{"location":"dokumentation/#theoretischer-hintergrund-ingress-und-service-routing","title":"Theoretischer Hintergrund: Ingress und Service Routing","text":"<p>Ingress ist eine Kubernetes Ressource zur Steuerung des externen Zugriffs auf Services im Cluster. Der Ingress Controller \u00fcbernimmt Routing, Host Matching und Weiterleitung an interne Services. Dadurch bleiben Pods und Services von direktem externem Zugriff entkoppelt.</p>"},{"location":"dokumentation/#security-und-betrieb","title":"Security und Betrieb","text":"Bereich Umsetzung Offener Ausbau Secrets GitHub Secrets, EC2 SSH, Registry Zugriff Rotation und Least Privilege Probes Readiness und Liveness geplant Grenzwerte und Fehlerf\u00e4lle testen Logs Container Logs \u00fcber kubectl abrufbar Zentrales Logging optional Rollback \u00dcber Image Tags m\u00f6glich Strategie dokumentieren und \u00fcben"},{"location":"dokumentation/#branching-strategie-und-regeln","title":"Branching Strategie und Regeln","text":""},{"location":"dokumentation/#ziel","title":"Ziel","text":"<p>Die Branching Strategie stellt sicher, dass der main Branch jederzeit stabil ist und den Stand f\u00fcr Sprint Reviews, Demo und Abgabe abbildet.</p> <p>Die laufende Entwicklung erfolgte bewusst \u00fcber einen permanenten develop Branch, ohne zus\u00e4tzliche Feature Branches. Dieses vereinfachte Modell wurde gew\u00e4hlt, da es sich um ein Einzelprojekt handelt und der Fokus auf Nachvollziehbarkeit, Qualit\u00e4tssicherung und stabilen Releases liegt.</p>"},{"location":"dokumentation/#branches","title":"Branches","text":"<ul> <li>main</li> <li>Stabiler Stand f\u00fcr Sprint Reviews, Demo und Abgabe</li> <li>\u00c4nderungen erfolgen nur \u00fcber Pull Requests</li> <li> <p>Keine direkten Pushes, sofern Branch Regeln aktiv sind</p> </li> <li> <p>develop</p> </li> <li>Permanenter Arbeitsbranch f\u00fcr s\u00e4mtliche \u00c4nderungen</li> <li>Alle technischen, fachlichen und CI/CD-Anpassungen wurden hier umgesetzt</li> <li>develop wurde bewusst nicht gel\u00f6scht, sondern kontinuierlich weiterentwickelt</li> <li> <p>Der Merge nach main erfolgt ausschliesslich \u00fcber Pull Requests</p> </li> <li> <p>gh-pages</p> </li> <li>Enth\u00e4lt die generierte GitHub Pages Ausgabe</li> <li>Wird ausschliesslich durch den GitHub Actions Workflow aktualisiert</li> <li>Keine manuelle Bearbeitung</li> </ul>"},{"location":"dokumentation/#merge-flow","title":"Merge Flow","text":"<ul> <li>Feature Umsetzung</li> <li>features werden direkt auf develop erstellt</li> <li>Pull Request von develop nach main</li> <li> <p>Review und Checks, dann Merge</p> </li> <li> <p>Sprint Stand oder Release</p> </li> <li>Pull Request von develop nach main</li> <li>main wird nur gemerged, wenn der Stand stabil ist</li> </ul>"},{"location":"dokumentation/#pull-request-workflow","title":"Pull Request Workflow","text":"<p>Jede relevante \u00c4nderung wurde \u00fcber einen Pull Request von <code>develop</code> nach <code>main</code> integriert.</p> <p>Der Pull Request diente dabei als zentrales Qualit\u00e4tstor und Nachweis f\u00fcr sauberes Arbeiten:</p> <ul> <li>\u00c4nderungen sind klar beschrieben und nachvollziehbar</li> <li>Automatische Checks laufen vor dem Merge</li> <li>Fehlerhafte Builds oder Tests blockieren den Merge</li> <li>Jeder Merge ist zeitlich und inhaltlich dokumentiert</li> </ul> <p>Dieser Workflow stellt sicher, dass der produktive Branch nicht durch ungetestete oder fehlerhafte \u00c4nderungen beeintr\u00e4chtigt wird.</p>"},{"location":"dokumentation/#regeln-und-schutz","title":"Regeln und Schutz","text":"<ul> <li>main</li> <li>Force Push blockiert</li> <li>Direkte Pushes blockiert, Merge nur \u00fcber Pull Request</li> <li> <p>Status Checks m\u00fcssen bestehen, falls Workflows definiert sind</p> </li> <li> <p>develop</p> </li> <li>Direkte Pushes erlaubt, aber bevorzugt \u00fcber Pull Requests</li> <li> <p>Force Push blockiert, falls Regeln aktiv sind</p> </li> <li> <p>gh-pages</p> </li> <li>Branch ist gesch\u00fctzt</li> <li>Updates erfolgen nur \u00fcber den GitHub Actions Workflow</li> <li>Keine manuellen Pushes</li> </ul>"},{"location":"dokumentation/#dokumentation-und-github-pages","title":"Dokumentation und GitHub Pages","text":"<p>Die GitHub Pages Dokumentation wird aus dem Repository gebaut. Der Workflow wird nur ausgef\u00fchrt, wenn relevante Dateien ge\u00e4ndert wurden, zum Beispiel <code>docs</code> oder <code>mkdocs.yml</code>. Dadurch bleibt die Pages Ausgabe konsistent mit dem Stand auf main.</p>"},{"location":"dokumentation/#commit-konvention","title":"Commit Konvention","text":"<p>Konvention: <code>type(scope): message</code></p> <p>Beispiele: - docs(pm): add sprint 1 review and retrospective - docs(arch): add target architecture overview - ci(pages): enable docs deployment workflow - ci(cd): deploy to k3s on push to main - feat(k3s): add deployment and service manifests - fix(ci): correct ghcr image tag - chore: update dependencies</p>"},{"location":"dokumentation/#definition-of-done-fur-branching-doku","title":"Definition of Done f\u00fcr Branching Doku","text":"<ul> <li>Dokumentation ist committed und gepusht</li> <li>Merge Flow ist nachvollziehbar beschrieben</li> <li>Branch Regeln sind dokumentiert</li> </ul>"},{"location":"dokumentation/#technische-dokumentation-fortlaufend","title":"Technische Dokumentation (fortlaufend)","text":"<p>Repository: https://github.com/Cancani/geraeteausleihe-sem4 GitHub Pages: https://cancani.com/geraeteausleihe-sem4/  </p> <p>AWS Region: us-east-1 EC2 Public IP: 13.223.28.53 Externer Endpoint: http://geraeteausleihe.13.223.28.53.nip.io  </p>"},{"location":"dokumentation/#ziel-dieser-technischen-dokumentation","title":"Ziel dieser technischen Dokumentation","text":"<p>Diese technische Dokumentation beschreibt den vollst\u00e4ndigen Stand bis Ende Sprint 2. Die Dokumentation wird sich im Laufe des Projekts st\u00e4ndig ver\u00e4ndern. Fokus ist ein lauff\u00e4higer End to End Betrieb inklusive Service Code, Tests, Containerisierung, GHCR, AWS EC2, K3s, Kubernetes Ressourcen, GitHub Actions CI und CD sowie nachvollziehbaren Nachweisen.</p> <p>Alle Aussagen sind so strukturiert, dass sie mit Commands, Outputs oder Screenshots belegt werden k\u00f6nnen.</p>"},{"location":"dokumentation/#systemubersicht","title":"System\u00fcbersicht","text":""},{"location":"dokumentation/#zielbild","title":"Zielbild","text":"<pre><code>flowchart TD\n  subgraph Clients\n    PA[PowerApps Client]\n    BR[Browser]\n  end\n\n  subgraph GitHub\n    REPO[Repository]\n    ACT[GitHub Actions]\n    GHCR[GitHub Container Registry]\n    PAGES[GitHub Pages]\n  end\n\n  subgraph AWS\n    EC2[AWS EC2 Instanz]\n    K3S[K3s Cluster]\n    TRF[Traefik Ingress]\n    SVC[Flask Microservice geraeteausleihe]\n  end\n\n  PA --&gt;|HTTP| TRF\n  BR --&gt;|HTTP| TRF\n  TRF --&gt;|Service Routing| SVC\n\n  REPO --&gt; ACT\n  ACT --&gt;|Build und Push| GHCR\n  ACT --&gt;|kubectl apply, set image| K3S\n  GHCR --&gt;|pull image| SVC\n\n  REPO --&gt;|MkDocs Build| PAGES</code></pre> <p>Abbildung 26: Soll-Workflow mit automatisiertem CI/CD-Prozess \u00fcber GitHub Actions und AWS</p>"},{"location":"dokumentation/#komponenten","title":"Komponenten","text":"Komponente Aufgabe Hinweis PowerApps Konsument des Microservice Ruft den PDF Endpoint auf Flask Microservice REST API und PDF Ausgabe L\u00e4uft containerisiert GitHub Actions Build, Push, Deploy und Docs CI CD und GitHub Pages GHCR Container Images Versionierung \u00fcber Tags AWS EC2 Compute Basis K3s l\u00e4uft auf der Instanz K3s Kubernetes Distribution Single Node Betrieb Traefik Ingress HTTP Ingress Controller Routing auf Service nip.io DNS Convenience Hostname ohne eigene DNS Zone"},{"location":"dokumentation/#schnittstellen-und-endpunkte","title":"Schnittstellen und Endpunkte","text":"Endpoint Methode Zweck Erwartung <code>/</code> GET Basis Response Text Response <code>/healthz</code> GET Health Check 200 OK mit JSON <code>/pdf</code> GET PDF Ausgabe f\u00fcr PowerApps PDF Response mit Content Disposition Attachment <p>Externe Verifikation: <pre><code>curl -i http://geraeteausleihe.13.223.28.53.nip.io/\ncurl -i http://geraeteausleihe.13.223.28.53.nip.io/healthz\ncurl -I \"http://geraeteausleihe.13.223.28.53.nip.io/pdf?borrower=Test&amp;device=Notebook&amp;staff=TBZ\"\n</code></pre></p> <p></p> <p>Abbildung 27: Curl Commands Endpoints</p>"},{"location":"dokumentation/#repository-struktur","title":"Repository Struktur","text":""},{"location":"dokumentation/#struktur","title":"Struktur","text":"<p>Wichtige Bereiche im Repository:</p> <ul> <li><code>service/</code> Python Flask App</li> <li><code>service/test_api.py</code> Pytest Tests</li> <li><code>Dockerfile</code> Container Image Build</li> <li><code>.github/workflows/</code> Workflows f\u00fcr CI, CD, PR Checks, Docs</li> <li><code>k3s/</code> Kubernetes Manifeste (Namespace, Deployment, Service, Ingress)</li> <li><code>docs/</code> MkDocs Seiten</li> <li><code>mkdocs.yml</code> MkDocs Konfiguration</li> </ul>"},{"location":"dokumentation/#python-flask-service","title":"Python Flask Service","text":""},{"location":"dokumentation/#zweck","title":"Zweck","text":"<p>Der Service stellt eine REST API bereit und erzeugt PDFs f\u00fcr die Ger\u00e4teausleihe. Nutzung erfolgt durch PowerApps oder Browser. Der Service l\u00e4uft lokal und produktionsnah im Container mit gunicorn.</p>"},{"location":"dokumentation/#konfiguration","title":"Konfiguration","text":"<p>Umgebungsvariable: - <code>PORT</code> wird im Kubernetes Deployment auf <code>8080</code> gesetzt.</p>"},{"location":"dokumentation/#ausfuhrung-lokal","title":"Ausf\u00fchrung Lokal","text":"<pre><code>cd service\npython -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\npython app.py\n</code></pre>"},{"location":"dokumentation/#endpoints","title":"Endpoints","text":""},{"location":"dokumentation/#get","title":"GET /","text":"<p>Erwartung: - HTTP 200 - Text Response</p> <pre><code>curl -i http://localhost:8080/\n</code></pre> <p></p> <p>Abbildung 28: GET / Lokal</p>"},{"location":"dokumentation/#get-healthz","title":"GET /healthz","text":"<p>Erwartung: - HTTP 200 - JSON mit status und timestamp</p> <pre><code>curl -i http://localhost:8080/healthz\n</code></pre> <p></p> <p>Abbildung 29: Health Check Endpoint Verifikation</p>"},{"location":"dokumentation/#get-pdf","title":"GET /pdf","text":"<p>Erwartung: - HTTP 200 - PDF Response - Content Disposition Attachment</p> <pre><code>curl -I \"http://localhost:8080/pdf?borrower=Test&amp;device=Notebook&amp;staff=IT\"\n</code></pre> <p></p> <p>Abbildung 30: PDF Generator Endpoint Test</p>"},{"location":"dokumentation/#tests","title":"Tests","text":""},{"location":"dokumentation/#ziel_1","title":"Ziel","text":"<p>Automatisierte Tests pr\u00fcfen die wichtigsten API Endpoints. Fokus liegt auf Response Codes, Content Type und Basis Inhalt.</p>"},{"location":"dokumentation/#besonderheiten-und-losung","title":"Besonderheiten und L\u00f6sung","text":"<p>WeasyPrint ben\u00f6tigt System Libraries. Lokale Tests auf Windows waren dadurch aufwendig. L\u00f6sung war, die Tests im Docker Container auszuf\u00fchren.</p> <p>Weitere Besonderheit: - Bytes Assertions mit Umlauten f\u00fchrten zu Syntax Problemen - L\u00f6sung: <code>response.get_data(as_text=True)</code></p>"},{"location":"dokumentation/#testausfuhrung-im-container","title":"Testausf\u00fchrung im Container","text":"<pre><code>docker build -t geraeteausleihe:test .\ndocker run --rm geraeteausleihe:test sh -c \"pip install pytest &amp;&amp; cd /srv &amp;&amp; pytest -q -p no:cacheprovider test_api.py\"\n</code></pre> <p>Erwartung: - Alle Tests erfolgreich,  <code>5 passed</code></p> <p></p> <p>Abbildung 31: Unit Tests mit Pytest</p>"},{"location":"dokumentation/#containerisierung","title":"Containerisierung","text":""},{"location":"dokumentation/#dockerfile-ziel","title":"Dockerfile Ziel","text":"<p>Das Dockerfile baut ein Image auf Basis <code>python:3.11-slim</code>. Es installiert WeasyPrint Abh\u00e4ngigkeiten (pango, cairo, gdk pixbuf und weitere) und startet den Service mit gunicorn.</p>"},{"location":"dokumentation/#lokaler-build-und-run","title":"Lokaler Build und Run","text":"<p>Build: <pre><code>docker build -t geraeteausleihe:local .\n</code></pre></p> <p>Test im Container: <pre><code>docker run --rm -p 8080:8080 geraeteausleihe:local\n</code></pre></p> <p></p> <p>Abbildung 32: Docker Container Verifikation</p> <p>Verifikation: <pre><code>curl -i http://localhost:8080/healthz\n</code></pre></p> <p></p> <p>Abbildung 33: Verifikation Container</p>"},{"location":"dokumentation/#github-container-registry-ghcr","title":"GitHub Container Registry GHCR","text":""},{"location":"dokumentation/#naming-und-tagging","title":"Naming und Tagging","text":"<p>Tags: - <code>latest</code> - Commit SHA, zum Beispiel <code>115b431876813ef6867c26de7e7cf6df0c533809</code></p> <p></p> <p>Abbildung 34: GitHub Container Registry Tags</p>"},{"location":"dokumentation/#aws-setup","title":"AWS Setup","text":""},{"location":"dokumentation/#ec2-instanz","title":"EC2 Instanz","text":"<p>Konfiguration: - Region: us east 1 - OS: Ubuntu 22.04 - Public IP: 13.223.28.53 - Elastic IP verwendet</p> <p></p>"},{"location":"dokumentation/#security-group","title":"Security Group","text":"<p>Inbound Rules: - 22 SSH - 80 HTTP - optional 443 sp\u00e4ter</p> <p>Neben den Inbound Regeln der Security Group wurden folgende Host und Zugriffs Massnahmen umgesetzt bzw. als Standard definiert, damit der Zugriff auf die EC2 Instanz kontrolliert bleibt.</p>"},{"location":"dokumentation/#host-hardening-und-zugriff","title":"Host Hardening und Zugriff","text":"<ol> <li>SSH Zugriff erfolgt mit Key Authentisierung, kein Passwort Login</li> <li>Root Login ist deaktiviert, Administration erfolgt \u00fcber einen dedizierten Benutzer mit sudo</li> <li>SSH ist auf definierte Quell IP Adressen eingeschr\u00e4nkt, sofern im jeweiligen Netzwerk m\u00f6glich</li> <li>Offen nach aussen sind nur die zwingend ben\u00f6tigten Ports f\u00fcr SSH und HTTP Ingress Traffic</li> <li>Applikations Pods sind nicht direkt exponiert, Zugriff erfolgt nur \u00fcber Ingress via Traefik</li> </ol>"},{"location":"dokumentation/#abgrenzung-demo-setup-und-produktiver-betrieb","title":"Abgrenzung Demo Setup und produktiver Betrieb","text":"<p>Dieses Setup ist als Lern und Demo Umgebung ausgelegt. Folgende Punkte sind bewusst nicht umgesetzt und m\u00fcssen f\u00fcr Produktion erg\u00e4nzt werden.</p> <ol> <li>TLS ist nicht aktiviert, da nip.io verwendet wird und der Fokus auf der Deploy Nachvollziehbarkeit liegt    F\u00fcr Produktion ist TLS via cert manager und validem DNS zwingend</li> <li>Kein umfassendes Cluster Hardening wie Network Policies oder Pod Security Standards, da Single Node K3s    F\u00fcr Produktion sollten mindestens Pod Security, Namespace Isolation und Network Policies bewertet werden</li> <li>Secrets Management ist im Projekt schlank gehalten    F\u00fcr Produktion w\u00e4re ein dediziertes Secret Management wie SOPS oder ein Vault Ansatz sinnvoll</li> </ol>"},{"location":"dokumentation/#k3s-installation-und-cluster-zugriff","title":"K3s Installation und Cluster Zugriff","text":""},{"location":"dokumentation/#k3s-installation","title":"K3s Installation","text":"<p>K3s wurde manuell installiert. Traefik ist standardm\u00e4ssig aktiv.</p> <p>Nachweise: <pre><code>sudo systemctl status k3s\nkubectl get nodes\nkubectl get pods -A\n</code></pre></p> <p></p> <p>Abbildung 35: K3s Cluster Status</p> <p></p> <p>Abbildung 36: kubectl Kommandozeilenausgabe</p>"},{"location":"dokumentation/#kubernetes-ressourcen","title":"Kubernetes Ressourcen","text":"<p>Die App wird \u00fcber deklarative YAML Manifeste in K3s betrieben. Die Ressourcen sind versioniert im Repository und werden durch GitHub Actions auf die EC2 Instanz ausgerollt.</p> Manifest Ressource Zweck Wichtige Punkte <code>k3s/namespace.yaml</code> Namespace Logische Trennung der App Ressourcen eigener Namespace f\u00fcr bessere \u00dcbersicht <code>k3s/deployment.yaml</code> Deployment Pods, Rolling Update, Probes Image Tag, Readiness und Liveness auf <code>/healthz</code> <code>k3s/service.yaml</code> Service ClusterIP Interner Zugriff stabile interne Adresse f\u00fcr die Pods <code>k3s/ingress.yaml</code> Ingress Externer Zugriff \u00fcber Traefik Host Routing \u00fcber nip.io"},{"location":"dokumentation/#theoretischer-hintergrund-kubernetes-und-deklarative-ressourcen","title":"Theoretischer Hintergrund: Kubernetes und deklarative Ressourcen","text":"<p>Kubernetes ist eine Plattform zur Orchestrierung containerisierter Anwendungen. Zentrale Konzepte sind deklarative Ressourcen, Self Healing Mechanismen und Rolling Updates. Der gew\u00fcnschte Zustand wird beschrieben, die Plattform sorgt selbstst\u00e4ndig f\u00fcr dessen Einhaltung.</p>"},{"location":"dokumentation/#namespace","title":"Namespace","text":"<p>Anwendung:</p> <pre><code>kubectl apply -f k3s/namespace.yaml\nkubectl get ns geraeteausleihe\n</code></pre>"},{"location":"dokumentation/#deployment","title":"Deployment","text":"<p>Anwendung und Kontrolle:</p> <pre><code>kubectl apply -f k3s/deployment.yaml\nkubectl -n geraeteausleihe rollout status deployment/geraeteausleihe\nkubectl -n geraeteausleihe get pods -o wide\n</code></pre> <p>Wesentliche Konfiguration im Deployment:</p> Element Wert Container Port 8080 Environment Variable PORT 8080 Readiness Probe GET <code>/healthz</code> Liveness Probe GET <code>/healthz</code> Image Pull Policy Always <p></p> <p>Abbildung 37: Kubernetes Deployment Konfiguration</p>"},{"location":"dokumentation/#service","title":"Service","text":"<p>Anwendung und Kontrolle:</p> <pre><code>kubectl apply -f k3s/service.yaml\nkubectl -n geraeteausleihe get svc\n</code></pre> <p></p> <p>Abbildung 38: Kubernetes Service Konfiguration</p>"},{"location":"dokumentation/#ingress","title":"Ingress","text":"<p>Anwendung und Kontrolle:</p> <pre><code>kubectl apply -f k3s/ingress.yaml\nkubectl -n geraeteausleihe get ingress\n</code></pre> <p>Ingress Host: <code>geraeteausleihe.&lt;EC2_IP&gt;.nip.io</code></p> <p></p> <p>Abbildung 39: Kubernetes Ingress Routing</p>"},{"location":"dokumentation/#github-actions","title":"GitHub Actions","text":""},{"location":"dokumentation/#workflow-ubersicht","title":"Workflow \u00dcbersicht","text":"<p>Folgende Workflows sind im Einsatz:</p> <ul> <li>container-build.yml: Build und Push nach GHCR</li> <li>deploy-k3s.yml: Deployment auf EC2 und Rollout Check</li> <li>pr-checks.yml: Build, Lint, Tests f\u00fcr PR nach main</li> <li>docs-pages.yml: MkDocs Build und Deploy nach gh-pages bei Docs \u00c4nderungen auf main</li> </ul> <p></p> <p>Abbildung 40: Workflows</p>"},{"location":"dokumentation/#pull-request-checks-continuous-integration","title":"Pull Request Checks (Continuous Integration)","text":"<p>F\u00fcr Pull Requests nach <code>main</code> ist ein separater CI Workflow definiert. Dieser wird automatisch bei jedem Pull Request ausgel\u00f6st und pr\u00fcft den aktuellen Stand vor dem Merge.</p> <p>Der Workflow umfasst:</p> <ul> <li>Build Pr\u00fcfung</li> <li>Linting</li> <li>Automatisierte Tests</li> </ul> <p>Nur wenn alle Checks erfolgreich sind, kann der Pull Request gemerged werden. Dadurch wird sichergestellt, dass der <code>main</code>-Branch jederzeit einen lauff\u00e4higen und gepr\u00fcften Stand enth\u00e4lt.</p>"},{"location":"dokumentation/#benotigte-secrets","title":"Ben\u00f6tigte Secrets","text":"<p>Erforderlich: - EC2_HOST - EC2_USER - EC2_SSH_KEY</p> <p></p> <p>Abbildung 41: GitHub Secrets Konfiguration</p>"},{"location":"dokumentation/#deployment-verifikation","title":"Deployment Verifikation","text":""},{"location":"dokumentation/#kubernetes-status","title":"Kubernetes Status","text":"<p><pre><code>kubectl -n geraeteausleihe get all\nkubectl -n geraeteausleihe get ingress\n</code></pre> </p> <p>Abbildung 42: Kubernetes Status</p>"},{"location":"dokumentation/#externe-requests","title":"Externe Requests","text":"<pre><code>curl -f http://geraeteausleihe.13.223.28.53.nip.io/healthz\ncurl -I \"http://geraeteausleihe.13.223.28.53.nip.io/pdf?borrower=Test&amp;device=Notebook&amp;staff=IT\"\n</code></pre> <p>Abbildung 43: Externe Requests</p>"},{"location":"dokumentation/#observability","title":"Observability","text":"<p>Observability ist bewusst schlank gehalten und basiert auf Kubernetes Standardmitteln. Ziel ist, dass Betrieb und Fehlersuche ohne zus\u00e4tzliche Plattform Komponenten nachvollziehbar m\u00f6glich sind.</p>"},{"location":"dokumentation/#logging-und-events","title":"Logging und Events","text":"<ol> <li>Applikations Logs laufen nach stdout und stderr und werden \u00fcber Kubernetes bereitgestellt</li> <li>Logs pro Pod pr\u00fcfen</li> <li><code>kubectl -n &lt;namespace&gt; logs deploy/&lt;deployment-name&gt;</code></li> <li><code>kubectl -n &lt;namespace&gt; logs &lt;pod-name&gt; --previous</code> bei Neustarts</li> <li>Events und Status pr\u00fcfen</li> <li><code>kubectl -n &lt;namespace&gt; get pods</code></li> <li><code>kubectl -n &lt;namespace&gt; describe pod &lt;pod-name&gt;</code></li> <li>Typische Hinweise sind ImagePullBackOff, CrashLoopBackOff, OOMKilled oder fehlende Readiness</li> </ol>"},{"location":"dokumentation/#health-checks","title":"Health Checks","text":"<ol> <li>Der Service stellt einen Health Endpoint bereit f\u00fcr eine einfache Verf\u00fcgbarkeitspr\u00fcfung</li> <li>F\u00fcr stabilen Betrieb sind Readiness und Liveness Probes vorgesehen, damit Traffic nur an gesunde Pods geht und defekte Pods automatisch neu gestartet werden</li> </ol>"},{"location":"dokumentation/#monitoring-und-metriken","title":"Monitoring und Metriken","text":"<ol> <li>Ein zentrales Monitoring Stack ist im Demo Setup nicht integriert</li> <li>F\u00fcr produktiven Betrieb w\u00e4ren Prometheus und Grafana sinnvoll, inklusive Alerts f\u00fcr</li> <li>HTTP Fehlerquote</li> <li>Latenz</li> <li>Pod Restarts</li> <li>Ressourcen Auslastung</li> </ol>"},{"location":"dokumentation/#powerapps-integration","title":"PowerApps Integration","text":""},{"location":"dokumentation/#launch-auf-pdf-endpoint","title":"Launch auf PDF Endpoint","text":"<p>PowerApps ruft <code>/pdf</code> \u00fcber Launch auf und nutzt EncodeUrl f\u00fcr borrower, device und staff.</p> <p></p> <p>Abbildung 44: PDF Generator Endpoint Test</p> <p>Ausgegebene Quittung</p> <p></p> <p>Abbildung 45: PDF Generator Endpoint Test</p>"},{"location":"dokumentation/#ci-build-und-push-nach-ghcr","title":"CI: Build und Push nach GHCR","text":""},{"location":"dokumentation/#theoretischer-hintergrund-continuous-integration-und-continuous-deployment","title":"Theoretischer Hintergrund: Continuous Integration und Continuous Deployment","text":"<p>Continuous Integration beschreibt das automatische Bauen und Testen von Code\u00e4nderungen nach jedem Commit. Continuous Deployment erweitert diesen Prozess um das automatisierte Ausrollen der Anwendung in die Zielumgebung. Ziel ist eine schnelle, reproduzierbare und fehlerarme Bereitstellung von Software.</p>"},{"location":"dokumentation/#ziel_2","title":"Ziel","text":"<p>Das Image soll nur dann gebaut werden, wenn sich service oder Build Artefakte \u00e4ndern. Zus\u00e4tzlich soll das Image eindeutig versioniert werden, damit Rollback und Nachvollziehbarkeit m\u00f6glich sind.</p>"},{"location":"dokumentation/#trigger-logik","title":"Trigger Logik","text":"<p>Das Container Image wird nur gebaut, wenn sich Dateien \u00e4ndern, die den Build Kontext beeinflussen.</p> <ul> <li><code>Dockerfile</code></li> <li><code>service/**</code> (Applikationscode und Tests)</li> <li>Dependency Dateien im <code>service/</code> Kontext (zum Beispiel <code>requirements*.txt</code>, falls vorhanden)</li> <li><code>.github/workflows/container-build.yml</code></li> </ul> <p>\u00c4nderungen an Kubernetes Manifesten unter <code>k3s/**</code> l\u00f6sen keinen Container Build aus. Sie werden \u00fcber den Deploy Workflow ausgerollt.</p> <p>GHCR verlangt lowercase Repository Namen. Falls das Repository auf GitHub Grossbuchstaben enth\u00e4lt, muss der Image Tag vor dem Push in lowercase umgewandelt werden.</p> <p>Container Build: </p> <p></p> <p>Abbildung 46: Container Build</p> <p>Deploy auf EC2:</p> <p></p> <p>Abbildung 47: K3s Cluster Status</p>"},{"location":"dokumentation/#tagging-strategie","title":"Tagging Strategie","text":"<p>Es werden zwei Tags verwendet.</p> <ul> <li>latest als schneller Referenz Tag f\u00fcr den aktuellen Stand</li> <li>Commit SHA als deterministische Version, die exakt zu einem Commit passt</li> </ul> <p>Beispiel.</p> <p><pre><code>ghcr.io/cancani/geraeteausleihe-sem4:latest\nghcr.io/cancani/geraeteausleihe-sem4:&lt;commit_sha&gt;\n</code></pre> </p> <p>Abbildung 48: Tagging</p>"},{"location":"dokumentation/#cd-deployment-nach-k3s-auf-aws-ec2","title":"CD: Deployment nach K3s auf AWS EC2","text":""},{"location":"dokumentation/#ordnerstruktur-fur-manifeste","title":"Ordnerstruktur f\u00fcr Manifeste","text":"<p>Die Kubernetes Manifeste liegen im Repository im Ordner k3s.</p> <ul> <li>namespace.yaml</li> <li>deployment.yaml</li> <li>service.yaml</li> <li>ingress.yaml</li> </ul>"},{"location":"dokumentation/#deploy-ablauf","title":"Deploy Ablauf","text":"<p>Der Deploy Workflow setzt die Umgebung in folgender Reihenfolge.</p> <ol> <li>Kubernetes Manifeste auf die EC2 kopieren</li> <li>namespace, deployment, service und ingress anwenden</li> <li>Ingress Host auf den aktuellen nip.io Host setzen</li> <li>Deployment Image auf den passenden Commit Tag setzen</li> <li>Rollout pr\u00fcfen</li> </ol>"},{"location":"dokumentation/#haufiger-fehler-invalidimagename","title":"H\u00e4ufiger Fehler: InvalidImageName","text":"<p>Symptom.</p> <ul> <li>Pod Status InvalidImageName</li> <li>Deployment zeigt ein Image wie ghcr.io/: <p>Typische Ursache.</p> <ul> <li>Image String wurde aus einer leeren oder nicht verf\u00fcgbaren Variable zusammengesetzt</li> <li>Das kann passieren, wenn Variablen im SSH Script nicht so verf\u00fcgbar sind wie im GitHub Runner</li> </ul> <p>Sofortmassnahme auf der EC2.</p> <pre><code>kubectl -n geraeteausleihe get deploy geraeteausleihe -o jsonpath='{.spec.template.spec.containers[0].name}{\"\\n\"}{.spec.template.spec.containers[0].image}{\"\\n\"}'\n\nCN=$(kubectl -n geraeteausleihe get deploy geraeteausleihe -o jsonpath='{.spec.template.spec.containers[0].name}')\nkubectl -n geraeteausleihe set image deployment/geraeteausleihe ${CN}=ghcr.io/cancani/geraeteausleihe-sem4:latest\nkubectl -n geraeteausleihe rollout restart deployment/geraeteausleihe\nkubectl -n geraeteausleihe rollout status deployment/geraeteausleihe --timeout=180s\n</code></pre> <p>Langfristige L\u00f6sung im Workflow.</p> <ul> <li>Image String im Workflow so zusammensetzen, dass er nicht leer sein kann</li> <li>Deploy SHA sauber setzen, je nach Trigger</li> <li>workflow run nutzt head sha des Build Runs</li> <li>workflow dispatch nutzt den aktuellen github sha</li> </ul>"},{"location":"dokumentation/#ingress-host-validierung","title":"Ingress Host Validierung","text":"<p>Ingress Hosts m\u00fcssen RFC 1123 konform sein, also nur lowercase, Zahlen, Punkte und Bindestriche, und d\u00fcrfen nicht mit einem Platzhalter wie CHANGE_ME deployed werden.</p>"},{"location":"dokumentation/#verifikation-nach-dem-deployment","title":"Verifikation nach dem Deployment","text":""},{"location":"dokumentation/#kontrolle-auf-der-ec2","title":"Kontrolle auf der EC2","text":"<pre><code>kubectl -n geraeteausleihe get pods -o wide\nkubectl -n geraeteausleihe get deploy\nkubectl -n geraeteausleihe get ingress\nkubectl -n geraeteausleihe get deploy geraeteausleihe -o jsonpath='{.spec.template.spec.containers[0].image}{\"\\n\"}'\n</code></pre> <p>Screenshots:</p>"},{"location":"dokumentation/#betrieb","title":"Betrieb","text":"<p>Dieses Kapitel beschreibt den minimalen Betrieb f\u00fcr das Single Node K3s Setup auf AWS EC2. Fokus ist ein reproduzierbarer Ablauf f\u00fcr Deployment, Verifikation, Rollback und Troubleshooting.</p>"},{"location":"dokumentation/#deployment-ablauf_1","title":"Deployment Ablauf","text":"<ol> <li>Code \u00c4nderung wird in GitHub gepusht</li> <li>Container Image wird gebaut und nach GHCR gepusht, nur bei \u00c4nderungen im Service Kontext</li> <li>Deployment Workflow wendet die Manifeste aus <code>k3s/</code> auf der EC2 Instanz an</li> <li>Deployment wird auf das neue Image aktualisiert</li> </ol>"},{"location":"dokumentation/#rollback-vorgehen","title":"Rollback Vorgehen","text":"<p>Ein Rollback ist m\u00f6glich, falls ein Deployment fehlschl\u00e4gt oder die Applikation nicht mehr korrekt reagiert.</p> <ol> <li>Rollout Verlauf pr\u00fcfen <code>kubectl -n &lt;namespace&gt; rollout history deploy/&lt;deployment-name&gt;</code></li> <li>Rollback auf vorherige Revision ausf\u00fchren <code>kubectl -n &lt;namespace&gt; rollout undo deploy/&lt;deployment-name&gt;</code></li> <li>Status pr\u00fcfen bis das Deployment wieder Available ist <code>kubectl -n &lt;namespace&gt; rollout status deploy/&lt;deployment-name&gt;</code></li> <li>Funktionstest \u00fcber Ingress URL durchf\u00fchren und Logs kontrollieren</li> </ol>"},{"location":"dokumentation/#troubleshooting-checkliste","title":"Troubleshooting Checkliste","text":""},{"location":"dokumentation/#ingress-und-routing","title":"Ingress und Routing","text":"<ol> <li>Ingress Ressource existiert <code>kubectl -n &lt;namespace&gt; get ingress</code></li> <li>Hostname stimmt und zeigt auf die EC2 IP</li> <li>Traefik sieht die Route und liefert keine 404 oder 502 Fehler</li> </ol>"},{"location":"dokumentation/#service-und-endpoints","title":"Service und Endpoints","text":"<ol> <li>Service zeigt auf die korrekten Labels <code>kubectl -n &lt;namespace&gt; describe svc &lt;service-name&gt;</code></li> <li>Endpoints sind vorhanden <code>kubectl -n &lt;namespace&gt; get endpoints &lt;service-name&gt;</code></li> <li>Pods sind Ready, sonst wird kein Traffic geroutet</li> </ol>"},{"location":"dokumentation/#pod-fehler","title":"Pod Fehler","text":"<ol> <li>Logs pr\u00fcfen <code>kubectl -n &lt;namespace&gt; logs &lt;pod-name&gt;</code></li> <li>Events pr\u00fcfen <code>kubectl -n &lt;namespace&gt; describe pod &lt;pod-name&gt;</code></li> <li>Ressourcen Hinweise pr\u00fcfen, zum Beispiel OOMKilled oder Memory Pressure</li> </ol>"},{"location":"dokumentation/#registry-und-image-pull","title":"Registry und Image Pull","text":"<ol> <li>Image Name und Tag sind korrekt im Deployment gesetzt</li> <li>Image ist in GHCR vorhanden</li> <li>Bei ImagePullBackOff pr\u00fcfen, ob Pull Rechte und Secret korrekt sind</li> </ol>"},{"location":"dokumentation/#wartung-und-betriebshinweise","title":"Wartung und Betriebshinweise","text":"<ol> <li>Basis Image und Dependencies regelm\u00e4ssig aktualisieren</li> <li>GitHub Actions Workflows und verwendete Actions Versionen aktuell halten</li> <li>Der Service ist stateless, daher liegt der Fokus auf reproduzierbarem Deploy und Versionierung    F\u00fcr produktiven Betrieb sollten Secrets und Cluster State separat gesichert und verwaltet werden</li> </ol>"},{"location":"dokumentation/#app-demo","title":"App-Demo","text":"<p>Das folgende .gif zeigt die Ger\u00e4teausleihe und das erhalten einer Quittung durch die EC2-Instanz.</p> <p>Demo Ger\u00e4teausleihe</p> <p>Abbildung 49: Demo der Ger\u00e4teausleihe-Applikation mit PDF-Generierung</p>"},{"location":"dokumentation/#projektabnahme","title":"Projektabnahme","text":"<p>Die Projektabnahme erfolgte anhand der definierten Zielsetzungen und Erfolgskriterien der Semesterarbeit.</p> <p>Alle Kernanforderungen wurden erf\u00fcllt. Der Microservice wird automatisiert gebaut, versioniert und ohne manuelle Eingriffe auf dem Kubernetes Cluster betrieben. Die externe Erreichbarkeit sowie der stabile Betrieb wurden technisch verifiziert.</p> <p>Damit gilt das Projekt aus fachlicher und projektmethodischer Sicht als erfolgreich abgeschlossen.</p>"},{"location":"dokumentation/#fazit_3","title":"Fazit","text":""},{"location":"dokumentation/#projekterfolg","title":"Projekterfolg","text":"<p>Das Ziel dieser Semesterarbeit war die Cloud Native Transformation des bestehenden Ger\u00e4teausleihe Microservice. Der Service wurde containerisiert, automatisiert gebaut, in einer Container Registry versioniert und auf einem Kubernetes Cluster betrieben. Die externe Erreichbarkeit ist \u00fcber Traefik Ingress und nip.io umgesetzt, inklusive der relevanten Endpunkte healthz und pdf. Der Betrieb wurde mit konkreten Verifikationen belegt, sowohl \u00fcber Kubernetes Status als auch \u00fcber externe Requests. </p> <p>Zus\u00e4tzlich wurde die Dokumentation als zentrale Pr\u00fcfbasis \u00fcber GitHub Pages ausgebaut. Damit sind technische Nachweise, Architektur, Projektmanagement und Betrieb an einem Ort nachvollziehbar. Dieser Aspekt war entscheidend, damit der Projektstand nicht nur funktioniert, sondern f\u00fcr Dozenten und Stakeholder auch effizient \u00fcberpr\u00fcfbar ist. </p> <p>Die definierten Ziele der Semesterarbeit wurden anhand der messbaren Erfolgskriterien \u00fcberpr\u00fcft und vollst\u00e4ndig erreicht.</p>"},{"location":"dokumentation/#reflexion","title":"Reflexion","text":"<p>Die gr\u00f6sste Erkenntnis aus der Umsetzung war, dass technische Funktion allein nicht gen\u00fcgt. Ein professioneller Cloud Engineering Output entsteht erst, wenn Technik, Nachweise und Projekttransparenz zusammenpassen. Genau diese L\u00fccke wurde im Verlauf des Projekts sichtbar. Im Sprint Review Kontext wurde kritisiert, dass zeitweise Transparenz fehlte, zum Beispiel fehlender \u00dcberblick \u00fcber Ziele und Herausforderungen, ein nicht zug\u00e4ngliches Board und uneinheitliche Sprintl\u00e4ngen. Diese R\u00fcckmeldungen wurden aufgenommen und konkret korrigiert, indem der \u00dcberblick in der Dokumentation verbessert, der Board Link aktualisiert und Sprint Ziele klarer formuliert wurden. </p> <p>Technisch war die Automatisierung der Deploy Schritte ein wiederkehrender Knackpunkt. Besonders deutlich wurde, wie schnell eine Pipeline fehlschlagen kann, wenn Variablen oder Image Tags nicht deterministisch gesetzt sind. Ein Beispiel daf\u00fcr ist der Fehler InvalidImageName, der durch leere oder falsch zusammengesetzte Image Strings entstehen kann. Daraus habe ich gelernt, dass CI und CD nicht nur gebaut, sondern konsequent \u00fcber Rollout Status, Logs und externe Health Checks verifiziert werden m\u00fcssen. </p> <p>Methodisch hat sich die sprintbasierte Arbeitsweise als klarer Gewinn gezeigt. WIP Limit, Sch\u00e4tzungen und sichtbare Priorisierung erh\u00f6hen den Fokus und reduzieren Parallelit\u00e4t. Das Feedback von Corrado hat diesen Teil best\u00e4tigt und gleichzeitig klar gemacht, dass Priorit\u00e4ten und Sch\u00e4tzungen f\u00fcr Dritte direkt auf den Board Karten sichtbar sein sollen, damit der Projektstand ohne Klicks verstanden wird. Diese Erkenntnis nehme ich als verbindlichen Standard f\u00fcr k\u00fcnftige Projekte mit.</p>"},{"location":"dokumentation/#reflexion-zur-arbeitsweise-mit-git-und-gitops","title":"Reflexion zur Arbeitsweise mit Git und GitOps","text":"<p>Ein besonders positiver Aspekt dieser Semesterarbeit war die konsequente Arbeit mit Git, Pull Requests und automatisierten im Sinne eines GitOps Ansatzes. Die t\u00e4gliche Arbeit mit Branches, Commits und Pull Requests hat mir nicht nur technisch geholfen, sondern auch meine Arbeitsweise nachhaltig ver\u00e4ndert.</p> <p>Durch den permanenten <code>develop</code> Branch und den klar gesch\u00fctztem <code>main</code> Branch entstand eine saubere Trennung zwischen laufender Entwicklung und stabilem Stand. \u00c4nderungen wurden bewusst nicht direkt produktiv gemacht, sondern \u00fcber Pull Requests integriert. Dieser Schritt hat sich als zentraler Qualit\u00e4tsmechanismus erwiesen. Jeder Merge war nachvollziehbar, begr\u00fcndet und durch automatische Checks abgesichert.</p> <p>Besonders motiverend war die direkte R\u00fcckmeldung durch die Pull Request Checks. Build, Lint und Tests liefen automatisch und zeigten unmittelbar, ob ein Stand mergef\u00e4hig ist oder nicht. Fehler wurden dadurch fr\u00fch sichtbar und konnten gezielt behoben werden, bevor sie den produktiven Branch erreichten. Dieser Ablauf hat mir gezeigt, wie start Automatisierung die Qualit\u00e4t und Ruhe im Entwicklungsprozess erh\u00f6ht.</p> <p>Auch das Arbeiten mit klaren Commits hat sich als grosser Mehrwert gezeigt. Kleine, thematisch saubere Commits machten es einfach, \u00c4nderungen nachzuvollziehen, Fehlerquellen einzugrenzen und Fortschritte transparent zu dokumentieren. In Kombination mit klaren Commit Messages entstand eine Art technisches Logbuch, das jederzeit erkl\u00e4rt, warum eine \u00c4nderung vorgenommen wurde.</p> <p>Der GitOps Gedanke wurde im Projekt praktisch erlebbar. Der gesamte Systemzustand ist versioniert im Repository abgelegt. Insgesamt hat mir das Projekt gezeigt, dass Git, Pull Requests und CICD nicht nur Werkzeuge sind, sondern ein Arbeitsmodell darstelllen. Ein Modell, das Qualit\u00e4t erzwingt, Transparenz schafft und Vertrauen in den eigenen Code und Betrieb aufbaut. Diese Erkenntnis nehme ich als festen Standard f\u00fcr zuk\u00fcnftige Projekte mit, sowohl im Ausbildungs- als auch im beruflichen Umfeld.</p>"},{"location":"dokumentation/#technische-erkenntnisse","title":"Technische Erkenntnisse","text":"<p>Im Projekt wurden zentrale Cloud Engineering Konzepte praktisch umgesetzt und vertieft.</p> <ol> <li> <p>Deklaratives Kubernetes Deployment     Der Betrieb erfolgt \u00fcber versionierte Kubernetes Ressourcen wie Namespace, Deployment, Service und Ingress. Dadurch ist der Stand reproduzierbar und der Betrieb wird \u00fcber die Plattform Eigenschaften wie Rolling Update und Self Healing unterst\u00fctzt. Readiness und Liveness basieren auf dem healthz Endpunkt. </p> </li> <li> <p>CI und CD mit sauberer Trigger Logik    Der Container Build wird nur ausgel\u00f6st, wenn sich der Build Kontext \u00e4ndert, zum Beispiel Dockerfile oder Service Code. \u00c4nderungen an Kubernetes Manifesten l\u00f6sen keinen Build aus und werden \u00fcber den Deploy Prozess ausgerollt. Das reduziert unn\u00f6tige Builds und h\u00e4lt den Prozess effizient. </p> </li> <li> <p>Versionierung und Nachvollziehbarkeit \u00fcber Tags    Die Tagging Strategie mit latest und Commit SHA sorgt daf\u00fcr, dass jede ausgelieferte Version eindeutig einem Stand zugeordnet werden kann. Das ist die Basis f\u00fcr reproduzierbare Releases und Rollback F\u00e4higkeit. </p> </li> <li> <p>Betrieb und Fehlersuche als Runbook Denkweise    Neben der reinen Umsetzung wurde Betrieb dokumentiert, inklusive Rollback Vorgehen und Troubleshooting Checkliste f\u00fcr Ingress, Service Endpoints, Pod Fehler sowie Registry Pull Probleme. Dadurch ist das System nicht nur lauff\u00e4hig, sondern auch betreibbar. </p> </li> <li> <p>Security als bewusste Abgrenzung zwischen Demo und Produktion    Security Group Regeln und grundlegende Host Massnahmen sind dokumentiert. Gleichzeitig ist klar beschrieben, welche Themen bewusst nicht umgesetzt wurden, zum Beispiel TLS, umfassendes Cluster Hardening und erweitertes Secrets Management, und was in einer produktiven Umgebung erforderlich w\u00e4re. </p> </li> <li> <p>Observability pragmatisch und nachvollziehbar    Observability wurde bewusst schlank gehalten und basiert auf Kubernetes Standardmitteln wie Logs und Events. Ein zentrales Monitoring ist nicht Teil des Demo Setups, wird aber als sinnvoller Ausbaupunkt f\u00fcr Produktion benannt. </p> </li> </ol> <p>Zusammenfassend hat die Semesterarbeit meine Kompetenzen in den Bereichen Cloud Engineering, CI CD Automatisierung und projektorientierte Arbeitsweise deutlich vertieft. Besonders der Umgang mit Fehlern, Reviews und externem Feedback hat gezeigt, wie wichtig saubere Nachweise und Transparenz f\u00fcr professionelle technische Arbeit sind. Diese Erkenntnisse nehme ich als festen Standard f\u00fcr zuk\u00fcnftige Projekte mit.</p>"},{"location":"dokumentation/#ausblick","title":"Ausblick","text":"<p>Der aktuelle Stand eignet sich als stabile Lern und Demo Umgebung und kann gezielt weiter professionalisiert werden.</p> <ol> <li> <p>Projekttransparenz weiter erh\u00f6hen    Burndown Darstellung pr\u00fcfen und wenn m\u00f6glich erg\u00e4nzen. Priorit\u00e4t und Story Points sollen direkt auf den Karten sichtbar sein, danach Screenshot in die Dokumentation.</p> </li> <li> <p>Evidence schneller erfassbar machen    Eine sehr kurze Demo als GIF in die Dokumentation integrieren, damit der Effekt sofort sichtbar ist. Relevante Personen im Review Kontext sollen dabei explizit markiert werden.</p> </li> <li> <p>Risikomanagement weiterentwickeln    Die Risikomatrix wird laufend gepflegt. Zus\u00e4tzlich kann die Risiko Entwicklung \u00fcber Zeit visualisiert werden, damit Trends sichtbar sind und Massnahmen messbar werden. </p> </li> <li> <p>Technische H\u00e4rtung und Produktiv Betrieb vorbereiten    TLS Einbindung mit cert manager und einem g\u00fcltigen DNS Setup, erg\u00e4nzende Cluster Schutzmassnahmen wie Pod Security und Network Policies sowie ein zentrales Monitoring mit Prometheus und Grafana.</p> </li> </ol>"},{"location":"dokumentation/#abbildungsverzeichnis","title":"Abbildungsverzeichnis","text":"Nr. Abbildung Kapitel Typ 1 Ist-Workflow mit manuellen Build- und Deployment-Schritten Ist Zustand Mermaid Flowchart 2 Soll-Workflow mit automatisiertem CI/CD-Prozess \u00fcber GitHub Actions und AWS Soll Zustand Mermaid Flowchart 3 Sprint-\u00dcbersicht im GitHub Project Board Projektphasen und Meilensteine Screenshot 4 Priorisierung im GitHub Project Board Priorisierung Screenshot 5 Sprint 1 Meilensteine und Issues Sprint 1 Planung, Review und Retrospektive Screenshot 6 Abgeschlossene Tasks in Sprint 1 Sprint 1 Review Screenshot 7 GitHub Project Board Ansicht f\u00fcr Sprint 1 Sprint 1 Review Screenshot 8 Issue-Labels zur Kategorisierung Sprint 1 Review Screenshot 9 Meilenstein-\u00dcbersicht im Projekt Sprint 1 Review Screenshot 10 Issue-Template f\u00fcr strukturierte Erfassung Sprint 1 Review Screenshot 11 Issue-Template f\u00fcr strukturierte Erfassung Sprint 1 Review Screenshot 12 Starfish Retrospektive Sprint 1 Sprint 1 Retrospektive Screenshot 13 Sprint 2 Meilensteine und Issues Sprint 2 Planung Screenshot 14 Abgeschlossene Tasks in Sprint 2 Sprint 2 Review Screenshot 15 Starfish Retrospektive Sprint 2 Sprint 2 Retrospektive Screenshot 16 Sprint 3 Meilensteine und Issues Sprint 3 Planung Screenshot 17 Sprint 3 Meilensteine und Issues Sprint 3 Review Screenshot 18 Starfish Retrospektive Sprint 3 Sprint 3 Retrospektive Screenshot 19 GitHub Project Board Gesamtansicht Verwaltung der Aufgaben Screenshot 20 SWOT-Analyse f\u00fcr Projektstrategie SWOT-Analyse Screenshot 21 Use-Case-Diagramm der Systeminteraktionen Use-Case Diagramm Screenshot 22 Risikomatrix mit identifizierten Projektrisiken Risikomatrix Screenshot 23 Sequenzdiagramm des Sprint-Zyklus Deployment Ablauf Mermaid Sequence 24 Soll-Workflow mit automatisiertem CI/CD-Prozess \u00fcber GitHub Actions und AWS Flowchart LR Komponenten und Datenfluss von Entwic... Mermaid Flowchart 25 Prozessdiagramm Flowchart TB interne Kubernetes Struktur im Cluste... Mermaid Flowchart 26 Soll-Workflow mit automatisiertem CI/CD-Prozess \u00fcber GitHub Actions und AWS Zielbild Mermaid Flowchart 27 Curl Commands Endpoints Schnittstellen und Endpunkte Screenshot 28 GET / Lokal GET / Screenshot 29 Health Check Endpoint Verifikation GET /healthz Screenshot 30 PDF Generator Endpoint Test GET /pdf Screenshot 31 Unit Tests mit Pytest Testausf\u00fchrung im Container Screenshot 32 Docker Container Verifikation Lokaler Build und Run Screenshot 33 Verifikation Container Lokaler Build und Run Screenshot 34 GitHub Container Registry Tags Naming und Tagging Screenshot 35 K3s Cluster Status K3s Installation Screenshot 36 kubectl Kommandozeilenausgabe K3s Installation Screenshot 37 Kubernetes Deployment Konfiguration Deployment Screenshot 38 Kubernetes Service Konfiguration Service Screenshot 39 Kubernetes Ingress Routing Ingress Screenshot 40 Workflows Workflow \u00dcbersicht Screenshot 41 GitHub Secrets Konfiguration Ben\u00f6tigte Secrets Screenshot 42 Kubernetes Status Kubernetes Status Screenshot 43 Externe Requests Externe Requests Screenshot 44 PDF Generator Endpoint Test Launch auf PDF Endpoint Screenshot 45 PDF Generator Endpoint Test Launch auf PDF Endpoint Screenshot 46 Container Build Trigger Logik Screenshot 47 K3s Cluster Status Trigger Logik Screenshot 48 Tagging Tagging Strategie Screenshot 49 Demo der Ger\u00e4teausleihe-Applikation mit PDF-Generierung App-Demo GIF"},{"location":"dokumentation/#legende","title":"Legende","text":"<p>Dateitypen: - PNG: Statische Screenshots und Grafiken - Mermaid: Interaktive Diagramme (Flowcharts, Sequenzdiagramme, Graphen) - MP4 / GIF: Kurze Demo- oder End-to-End-Demonstrationen des Systems</p> <p>Mermaid Diagramme im Detail: - Flowchart: Prozessabl\u00e4ufe und Workflows - Graph TB/LR: Systemarchitektur und Use-Cases - Sequence: Datenfluss und API-Kommunikation</p> <p>Pfad-Referenzen: Alle Medien befinden sich im Verzeichnis <code>./screenshots/</code> relativ zur Dokumentation</p>"},{"location":"dokumentation/#glossar","title":"Glossar","text":"Begriff Erkl\u00e4rung AWS Amazon Web Services, Cloud Plattform f\u00fcr Infrastruktur EC2 Virtuelle Server Instanz innerhalb von AWS K3s Leichtgewichtige Kubernetes Distribution Kubernetes Plattform zur Orchestrierung von Containern Pod Kleinste ausf\u00fchrbare Einheit in Kubernetes Deployment Kubernetes Ressource f\u00fcr Rollout und Skalierung Service Kubernetes Ressource zur internen Netzwerkfreigabe Ingress Kubernetes Ressource f\u00fcr externen Zugriff auf Services Traefik Ingress Controller f\u00fcr Kubernetes nip.io DNS Dienst zur Nutzung von IP basierten Hostnamen Container Isolierte Laufzeitumgebung f\u00fcr Anwendungen Docker Tool zur Erstellung und Ausf\u00fchrung von Containern CI/CD Automatisierter Prozess f\u00fcr Build und Deployment GitHub Actions CI/CD Plattform innerhalb von GitHub GHCR GitHub Container Registry f\u00fcr Container Images Image Gebautes Container Artefakt Rollout Ausrollen einer neuen Version Rollback Zur\u00fccksetzen auf eine vorherige Version Health Check Technische Pr\u00fcfung der Service Verf\u00fcgbarkeit Microservice Eigenst\u00e4ndiger, containerisierter Backend Service Observability Sammelbegriff f\u00fcr Logs, Status und \u00dcberwachung Stateless Anwendung ohne persistenten Zustand YAML Konfigurationsformat f\u00fcr Kubernetes Manifeste"},{"location":"dokumentation/#kontakt","title":"Kontakt","text":"<p>F\u00fcr R\u00fcckfragen oder weiterf\u00fchrende Informationen zu diesem Projekt:</p> <p>Efekan Demirci Technische Berufsschule Z\u00fcrich (TBZ) Informatikdienst efekan@demirci.ch</p> <p>Die Projektdokumentation ist \u00f6ffentlich zug\u00e4nglich.</p>"}]}